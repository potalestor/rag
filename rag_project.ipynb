{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RAG project\n",
    "\n",
    "The RAG (Retrieval-Augmented Generation) project aims to explore and optimize various components of a RAG system."
   ],
   "id": "ae62b9f137ae27b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. Setup\n",
    "\n",
    "This infrastructure setup defines a multi-service environment for a RAG using Docker Compose, with some adjustments. The main components include:\n",
    "\n",
    "1. **Embedding Service (`embed`)**: Generates text embeddings using the pre-trained model `sentence-transformers/all-MiniLM-L6-v2`.\n",
    "2. **Reranking Service (`rerank`)**: Refines the ranking of text results using the `mixedbread-ai/mxbai-rerank-xsmall-v` model.\n",
    "3. **RAG Service (`rag_service`)**: Manages document retrieval and text generation, integrating both the embedding and reranking services.\n",
    "4. **Redis**: Provides an in-memory data store for caching or queuing.\n",
    "5. **GPT Inference Services (`gpt-4-mini` and `gpt-4o`)**: Run GPT-like models for generating responses, enabling advanced language capabilities.\n",
    "6. **Gateway Service (`gateway_service`)**: Acts as the central API gateway, managing interactions between all services.\n",
    "7. **PostgreSQL Database (`postgres`)**: Stores structured data such as metadata and logs.\n",
    "8. **Gradio UI (`rag_gradio_service`)**: Provides a user interface for easy interaction with the RAG pipeline."
   ],
   "id": "b96babc92ab57eda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T13:41:26.581726Z",
     "start_time": "2024-12-12T13:41:26.577110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "open_ai_key = open(\"keys/.openai-api-key\").read().strip()\n",
    "os.environ[\"OPENAI_API_KEY\"] = open_ai_key\n",
    "\n",
    "cohere_key = open(\"keys/.cohere-api-key\").read().strip()\n",
    "os.environ[\"COHERE_API_KEY\"] = cohere_key\n",
    "\n",
    "nebius_key = open(\"keys/.nebius-api-key\").read().strip()\n",
    "os.environ[\"NEBIUS_API_KEY\"] = nebius_key\n",
    "\n",
    "huggingface_key = open(\"keys/.huggingface-api-key\").read().strip()\n",
    "os.environ[\"HUG_API_KEY\"] = huggingface_key"
   ],
   "id": "f4a166f0ae934344",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%writefile .env\n",
    "MODEL_NAME=\"gpt-4\"\n",
    "ADMIN_KEY=\"aYpVtQxRmGzLsBnCfDiKjUxWqHvNwYcFbXlPrVdTw\"\n",
    "DATABASE_URL=\"postgresql://user:password@postgres/dbname\""
   ],
   "id": "b7e3bbf657e99797"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T22:40:06.279715Z",
     "start_time": "2024-12-11T22:40:05.887135Z"
    }
   },
   "cell_type": "code",
   "source": "!docker-compose -f docker-compose.yaml up -d",
   "id": "52c7e55b72b61395",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1A\u001B[1B\u001B[0G\u001B[?25l[+] Running 9/0\r\n",
      " \u001B[32m✔\u001B[0m Container inference_service_gpt4mini   \u001B[32mRunning\u001B[0m                          \u001B[34m0.0s \u001B[0m\r\n",
      " \u001B[32m✔\u001B[0m Container rerank_service               \u001B[32mRu...\u001B[0m                            \u001B[34m0.0s \u001B[0m\r\n",
      " \u001B[32m✔\u001B[0m Container embed_service                \u001B[32mRun...\u001B[0m                           \u001B[34m0.0s \u001B[0m\r\n",
      " \u001B[32m✔\u001B[0m Container inference_service_gpt4o      \u001B[32mRunning\u001B[0m                          \u001B[34m0.0s \u001B[0m\r\n",
      " \u001B[32m✔\u001B[0m Container rag_gradio_service           \u001B[32mRunning\u001B[0m                          \u001B[34m0.0s \u001B[0m\r\n",
      " \u001B[32m✔\u001B[0m Container redis                        \u001B[32mRunning\u001B[0m                          \u001B[34m0.0s \u001B[0m\r\n",
      " \u001B[32m✔\u001B[0m Container topic-1-advanced-postgres-1  \u001B[32mRunning\u001B[0m                          \u001B[34m0.0s \u001B[0m\r\n",
      " \u001B[32m✔\u001B[0m Container rag_service                  \u001B[32mRunni...\u001B[0m                         \u001B[34m0.0s \u001B[0m\r\n",
      " \u001B[32m✔\u001B[0m Container gateway_service              \u001B[32mR...\u001B[0m                             \u001B[34m0.0s \u001B[0m\r\n",
      "\u001B[?25h"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Database Setup and Text Extraction\n",
    "### Download files\n",
    "1. Clone the *Transformers* repository to your virtual machine:\n",
    "```bash\n",
    "git clone https://github.com/huggingface/transformers\n",
    "```\n",
    "2. Run the script to extract raw text from markdown files located in the transformers/docs/source/en/ directory:\n",
    "```bash\n",
    "python prep_scripts/markdown_to_text.py --input-dir transformers/docs/source/en/ --output-dir docs\n",
    "```\n",
    "3. The output will be a collection of plain text files stored in the docs directory.\n",
    "This directory will serve as our knowledge base.\n",
    "\n",
    "### Text Preprocessing and Summarization\n",
    "Only files that start with the prefix \"model\" and have a size no greater than 5 KB are selected. This is a model constraint. In ChatGPT, each of these files will be summarized.\n",
    "\n",
    "1. Remove Noise \n",
    "2. Preserve Key Information\n",
    "3. Summarize Text\n",
    "4. Text Segmentation"
   ],
   "id": "6667f236af5dcf1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T22:40:03.252486Z",
     "start_time": "2024-12-11T22:25:50.293994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# Define directories\n",
    "raw_dir = 'docs/raw'\n",
    "prepared_dir = 'docs/prepared'\n",
    "MAX_FILE_SIZE = 5120\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\" \n",
    "\n",
    "\n",
    "SYSTEM_MESSAGE = \"You are a helpful assistant specialized in summarizing texts.\"\n",
    "SUMMARY_PROMPT_TEMPLATE = \"Please summarize the following text. Each paragraph should contain no more than 50 tokens. Do not include headers, and split the text using \\n\\n:{}\"\n",
    "\n",
    "# read the content of a file\n",
    "def read_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()  \n",
    "    return content\n",
    "\n",
    "# send content to ChatGPT for processing\n",
    "def modify_content_with_chatgpt(content):    \n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "                {\"role\": \"user\", \"content\": SUMMARY_PROMPT_TEMPLATE.format(content)},\n",
    "            ]\n",
    "        )\n",
    "            \n",
    "        summary = response.choices[0].message.content\n",
    "        return summary.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"\"    \n",
    "\n",
    "\n",
    "# process files in the raw directory\n",
    "def process_files(raw_dir, prepared_dir):\n",
    "    if not os.path.exists(prepared_dir):\n",
    "        os.makedirs(prepared_dir)\n",
    "    \n",
    "    # iterate over files in the raw directory\n",
    "    for filename in os.listdir(raw_dir):\n",
    "        file_path = os.path.join(raw_dir, filename)\n",
    "        \n",
    "        # Step 1: Check if the file starts with 'model'\n",
    "        if filename.startswith('model') and os.path.getsize(file_path) <= MAX_FILE_SIZE:\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            \n",
    "            # Step 2: Read the content of the file\n",
    "            content = read_file(file_path)\n",
    "            \n",
    "            # Step 3: Modify the content using ChatGPT\n",
    "            modified_content = modify_content_with_chatgpt(content)\n",
    "            \n",
    "            # Step 4: Save the modified content to the 'prepared' directory\n",
    "            destination_file = os.path.join(prepared_dir, filename)\n",
    "            with open(destination_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(modified_content)\n",
    "            \n",
    "            print(f\"File '{filename}' processed and saved to '{prepared_dir}'.\")\n",
    "        else:\n",
    "            print(f\"File '{filename}' does not start with 'model'. Skipping...\")\n",
    "\n",
    "# Process the files\n",
    "process_files(raw_dir, prepared_dir)\n",
    "\n",
    "print(\"All files starting with 'model' have been processed and saved to 'docs/prepared'.\")"
   ],
   "id": "2baaa356e4f58798",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: model_doc_trocr.txt\n",
      "File 'model_doc_trocr.txt' processed and saved to 'docs/prepared'.\n",
      "File 'main_classes_model.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_llama2.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_masked_language_modeling.txt' does not start with 'model'. Skipping...\n",
      "File '_perf_infer_cpu.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_owlvit.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_levit.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_m2m_100.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_deberta-v2.txt\n",
      "File 'model_doc_deberta-v2.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_longt5.txt\n",
      "File 'model_doc_longt5.txt' processed and saved to 'docs/prepared'.\n",
      "File '_attention.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_gemma2.txt\n",
      "File 'model_doc_gemma2.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_xclip.txt\n",
      "File 'model_doc_xclip.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_xlm-prophetnet.txt\n",
      "File 'model_doc_xlm-prophetnet.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_megatron_gpt2.txt\n",
      "File 'model_doc_megatron_gpt2.txt' processed and saved to 'docs/prepared'.\n",
      "File '_installation.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_omdet-turbo.txt' does not start with 'model'. Skipping...\n",
      "File '_model_summary.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_blip.txt\n",
      "File 'model_doc_blip.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_convnextv2.txt\n",
      "File 'model_doc_convnextv2.txt' processed and saved to 'docs/prepared'.\n",
      "File '_tokenizer_summary.txt' does not start with 'model'. Skipping...\n",
      "File '_tiktoken.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_open-llama.txt\n",
      "File 'model_doc_open-llama.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_image_text_to_text.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_llama3.txt\n",
      "File 'model_doc_llama3.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_distilbert.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_layoutxlm.txt\n",
      "File 'model_doc_layoutxlm.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_seggpt.txt\n",
      "File 'model_doc_seggpt.txt' processed and saved to 'docs/prepared'.\n",
      "File '_perf_torch_compile.txt' does not start with 'model'. Skipping...\n",
      "File '_perf_hardware.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_trajectory_transformer.txt\n",
      "File 'model_doc_trajectory_transformer.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_pegasus_x.txt\n",
      "File 'model_doc_pegasus_x.txt' processed and saved to 'docs/prepared'.\n",
      "File '_multilingual.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_granitemoe.txt\n",
      "File 'model_doc_granitemoe.txt' processed and saved to 'docs/prepared'.\n",
      "File '_task_summary.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_bert-japanese.txt\n",
      "File 'model_doc_bert-japanese.txt' processed and saved to 'docs/prepared'.\n",
      "File 'main_classes_output.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_nougat.txt\n",
      "File 'model_doc_nougat.txt' processed and saved to 'docs/prepared'.\n",
      "File 'internal_file_utils.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_reformer.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_granite.txt\n",
      "File 'model_doc_granite.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_roberta-prelayernorm.txt\n",
      "File 'model_doc_roberta-prelayernorm.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_video_text_to_text.txt' does not start with 'model'. Skipping...\n",
      "File '__config.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_informer.txt\n",
      "File 'model_doc_informer.txt' processed and saved to 'docs/prepared'.\n",
      "File '_model_sharing.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_clap.txt\n",
      "File 'model_doc_clap.txt' processed and saved to 'docs/prepared'.\n",
      "File '_gguf.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_gpt_neo.txt\n",
      "File 'model_doc_gpt_neo.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_flava.txt\n",
      "File 'model_doc_flava.txt' processed and saved to 'docs/prepared'.\n",
      "File '_generation_strategies.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_gpt2.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_univnet.txt\n",
      "File 'model_doc_univnet.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_llava_next.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_opt.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_swiftformer.txt\n",
      "File 'model_doc_swiftformer.txt' processed and saved to 'docs/prepared'.\n",
      "File 'main_classes_logging.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_glpn.txt\n",
      "File 'model_doc_glpn.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_vit.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_hubert.txt\n",
      "File 'model_doc_hubert.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_perceiver.txt' does not start with 'model'. Skipping...\n",
      "File '_llm_tutorial.txt' does not start with 'model'. Skipping...\n",
      "File '_autoclass_tutorial.txt' does not start with 'model'. Skipping...\n",
      "File '_agents.txt' does not start with 'model'. Skipping...\n",
      "File 'internal_image_processing_utils.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_xmod.txt\n",
      "File 'model_doc_xmod.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_musicgen.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_bros.txt' does not start with 'model'. Skipping...\n",
      "File '_philosophy.txt' does not start with 'model'. Skipping...\n",
      "File '_benchmarks.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_timesformer.txt\n",
      "File 'model_doc_timesformer.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_clipseg.txt\n",
      "File 'model_doc_clipseg.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_dinov2.txt\n",
      "File 'model_doc_dinov2.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_barthez.txt\n",
      "File 'model_doc_barthez.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_madlad-400.txt\n",
      "File 'model_doc_madlad-400.txt' processed and saved to 'docs/prepared'.\n",
      "File '_accelerate.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_ibert.txt\n",
      "File 'model_doc_ibert.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_image_captioning.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_musicgen_melody.txt' does not start with 'model'. Skipping...\n",
      "File 'quantization_optimum.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_sam.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_big_bird.txt\n",
      "File 'model_doc_big_bird.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_bridgetower.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_layoutlm.txt\n",
      "File 'model_doc_layoutlm.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_matcha.txt\n",
      "File 'model_doc_matcha.txt' processed and saved to 'docs/prepared'.\n",
      "File '_model_memory_anatomy.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_image_to_image.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_cpm.txt\n",
      "File 'model_doc_cpm.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_owlv2.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_tapas.txt' does not start with 'model'. Skipping...\n",
      "File '_sagemaker.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_qdqbert.txt' does not start with 'model'. Skipping...\n",
      "File '_performance.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_align.txt\n",
      "File 'model_doc_align.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_roberta.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_idefics.txt\n",
      "File 'model_doc_idefics.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_mctct.txt\n",
      "File 'model_doc_mctct.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_depth_anything.txt\n",
      "File 'model_doc_depth_anything.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_mgp-str.txt\n",
      "File 'model_doc_mgp-str.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_swin.txt\n",
      "File 'model_doc_swin.txt' processed and saved to 'docs/prepared'.\n",
      "File 'main_classes_tokenizer.txt' does not start with 'model'. Skipping...\n",
      "File '_conversations.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_jetmoe.txt\n",
      "File 'model_doc_jetmoe.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_text-to-speech.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_udop.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_convbert.txt\n",
      "File 'model_doc_convbert.txt' processed and saved to 'docs/prepared'.\n",
      "File 'main_classes_onnx.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_data2vec.txt\n",
      "File 'model_doc_data2vec.txt' processed and saved to 'docs/prepared'.\n",
      "File '_big_models.txt' does not start with 'model'. Skipping...\n",
      "File 'internal_time_series_utils.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_mpt.txt\n",
      "File 'model_doc_mpt.txt' processed and saved to 'docs/prepared'.\n",
      "File '_chat_templating.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_herbert.txt\n",
      "File 'model_doc_herbert.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_retribert.txt\n",
      "File 'model_doc_retribert.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_mamba.txt\n",
      "File 'model_doc_mamba.txt' processed and saved to 'docs/prepared'.\n",
      "File 'main_classes_trainer.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_dac.txt\n",
      "File 'model_doc_dac.txt' processed and saved to 'docs/prepared'.\n",
      "File '_serialization.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_xlsr_wav2vec2.txt\n",
      "File 'model_doc_xlsr_wav2vec2.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_mllama.txt\n",
      "File 'model_doc_mllama.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_qwen2_moe.txt\n",
      "File 'model_doc_qwen2_moe.txt' processed and saved to 'docs/prepared'.\n",
      "File 'quantization_compressed_tensors.txt' does not start with 'model'. Skipping...\n",
      "File '_notebooks.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_fastspeech2_conformer.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_transfo-xl.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_flan-t5.txt\n",
      "File 'model_doc_flan-t5.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_umt5.txt\n",
      "File 'model_doc_umt5.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_groupvit.txt\n",
      "File 'model_doc_groupvit.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_xlm-roberta.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_paligemma.txt\n",
      "File 'model_doc_paligemma.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_stablelm.txt\n",
      "File 'model_doc_stablelm.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_falcon_mamba.txt\n",
      "File 'model_doc_falcon_mamba.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_qwen2_vl.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_sequence_classification.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_blip-2.txt\n",
      "File 'model_doc_blip-2.txt' processed and saved to 'docs/prepared'.\n",
      "File '_perf_infer_gpu_multi.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_pvt.txt\n",
      "File 'model_doc_pvt.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_led.txt\n",
      "File 'model_doc_led.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_videomae.txt\n",
      "File 'model_doc_videomae.txt' processed and saved to 'docs/prepared'.\n",
      "File '_tasks_explained.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_ernie_m.txt\n",
      "File 'model_doc_ernie_m.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_prophetnet.txt\n",
      "File 'model_doc_prophetnet.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_tvp.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_whisper.txt' does not start with 'model'. Skipping...\n",
      "File '_add_new_pipeline.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_bart.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_pop2piano.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_graphormer.txt\n",
      "File 'model_doc_graphormer.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_sew.txt\n",
      "File 'model_doc_sew.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_deformable_detr.txt\n",
      "File 'model_doc_deformable_detr.txt' processed and saved to 'docs/prepared'.\n",
      "File 'main_classes_agent.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_camembert.txt\n",
      "File 'model_doc_camembert.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_funnel.txt\n",
      "File 'model_doc_funnel.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_chameleon.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_language_modeling.txt' does not start with 'model'. Skipping...\n",
      "File '_torchscript.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_pix2struct.txt\n",
      "File 'model_doc_pix2struct.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_bartpho.txt\n",
      "File 'model_doc_bartpho.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_van.txt\n",
      "File 'model_doc_van.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_flaubert.txt\n",
      "File 'model_doc_flaubert.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_mvp.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_siglip.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_prompting.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_oneformer.txt\n",
      "File 'model_doc_oneformer.txt' processed and saved to 'docs/prepared'.\n",
      "File '_perf_train_gpu_one.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_asr.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_byt5.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_wav2vec2.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_vit_msn.txt\n",
      "File 'model_doc_vit_msn.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_fnet.txt\n",
      "File 'model_doc_fnet.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_swinv2.txt\n",
      "File 'model_doc_swinv2.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_blenderbot.txt\n",
      "File 'model_doc_blenderbot.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_dialogpt.txt\n",
      "File 'model_doc_dialogpt.txt' processed and saved to 'docs/prepared'.\n",
      "File 'quantization_contribute.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_encoder-decoder.txt' does not start with 'model'. Skipping...\n",
      "File 'quantization_eetq.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_bertweet.txt\n",
      "File 'model_doc_bertweet.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_detr.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_rt_detr.txt\n",
      "File 'model_doc_rt_detr.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_llava.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_zamba.txt\n",
      "File 'model_doc_zamba.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_mobilevit.txt\n",
      "File 'model_doc_mobilevit.txt' processed and saved to 'docs/prepared'.\n",
      "File '_perf_train_cpu_many.txt' does not start with 'model'. Skipping...\n",
      "File 'quantization_quanto.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_knowledge_distillation_for_image_classification.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_altclip.txt\n",
      "File 'model_doc_altclip.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_deta.txt\n",
      "File 'model_doc_deta.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_llava_next_video.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_jukebox.txt\n",
      "File 'model_doc_jukebox.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_canine.txt\n",
      "File 'model_doc_canine.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_mluke.txt\n",
      "File 'model_doc_mluke.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_speecht5.txt\n",
      "File 'model_doc_speecht5.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_roformer.txt\n",
      "File 'model_doc_roformer.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_pegasus.txt\n",
      "File 'model_doc_pegasus.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_nystromformer.txt\n",
      "File 'model_doc_nystromformer.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_focalnet.txt\n",
      "File 'model_doc_focalnet.txt' processed and saved to 'docs/prepared'.\n",
      "File '_community.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_token_classification.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_segformer.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_vision-text-dual-encoder.txt\n",
      "File 'model_doc_vision-text-dual-encoder.txt' processed and saved to 'docs/prepared'.\n",
      "File 'quantization_awq.txt' does not start with 'model'. Skipping...\n",
      "File '_bertology.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_zoedepth.txt' does not start with 'model'. Skipping...\n",
      "File 'main_classes_pipelines.txt' does not start with 'model'. Skipping...\n",
      "File '_testing.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_biogpt.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_unispeech-sat.txt\n",
      "File 'model_doc_unispeech-sat.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_albert.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_markuplm.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_qwen2_audio.txt' does not start with 'model'. Skipping...\n",
      "File 'main_classes_configuration.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_mamba2.txt\n",
      "File 'model_doc_mamba2.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_pixtral.txt\n",
      "File 'model_doc_pixtral.txt' processed and saved to 'docs/prepared'.\n",
      "File 'quantization_torchao.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_splinter.txt\n",
      "File 'model_doc_splinter.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_sew-d.txt\n",
      "File 'model_doc_sew-d.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_phobert.txt\n",
      "File 'model_doc_phobert.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_yolos.txt\n",
      "File 'model_doc_yolos.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_tapex.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_idefics2.txt' does not start with 'model'. Skipping...\n",
      "File '_pr_checks.txt' does not start with 'model'. Skipping...\n",
      "File 'internal_tokenization_utils.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_visual_question_answering.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_bert.txt' does not start with 'model'. Skipping...\n",
      "File '_create_a_model.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_chinese_clip.txt\n",
      "File 'model_doc_chinese_clip.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_recurrent_gemma.txt\n",
      "File 'model_doc_recurrent_gemma.txt' processed and saved to 'docs/prepared'.\n",
      "File 'quantization_aqlm.txt' does not start with 'model'. Skipping...\n",
      "File '_debugging.txt' does not start with 'model'. Skipping...\n",
      "File 'internal_modeling_utils.txt' does not start with 'model'. Skipping...\n",
      "File 'main_classes_deepspeed.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_idefics3.txt\n",
      "File 'model_doc_idefics3.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_gpt_neox_japanese.txt\n",
      "File 'model_doc_gpt_neox_japanese.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_phi3.txt\n",
      "File 'model_doc_phi3.txt' processed and saved to 'docs/prepared'.\n",
      "File '_agents_advanced.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_convnext.txt\n",
      "File 'model_doc_convnext.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_blenderbot-small.txt\n",
      "File 'model_doc_blenderbot-small.txt' processed and saved to 'docs/prepared'.\n",
      "File 'quantization_overview.txt' does not start with 'model'. Skipping...\n",
      "File 'main_classes_image_processor.txt' does not start with 'model'. Skipping...\n",
      "File '_custom_models.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_bark.txt' does not start with 'model'. Skipping...\n",
      "File '_perf_train_tpu_tf.txt' does not start with 'model'. Skipping...\n",
      "File '_quicktour.txt' does not start with 'model'. Skipping...\n",
      "File '_llm_optims.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_summarization.txt' does not start with 'model'. Skipping...\n",
      "File '_perf_train_cpu.txt' does not start with 'model'. Skipping...\n",
      "File 'main_classes_quantization.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_rwkv.txt' does not start with 'model'. Skipping...\n",
      "File 'README.md' does not start with 'model'. Skipping...\n",
      "File '_hpo_train.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_visual_bert.txt\n",
      "File 'model_doc_visual_bert.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_audio_classification.txt' does not start with 'model'. Skipping...\n",
      "File 'main_classes_callback.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_lxmert.txt\n",
      "File 'model_doc_lxmert.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_conditional_detr.txt\n",
      "File 'model_doc_conditional_detr.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_zero_shot_object_detection.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_resnet.txt\n",
      "File 'model_doc_resnet.txt' processed and saved to 'docs/prepared'.\n",
      "File '_trainer.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_speech-encoder-decoder.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_t5v1.1.txt\n",
      "File 'model_doc_t5v1.1.txt' processed and saved to 'docs/prepared'.\n",
      "File 'quantization_gptq.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_translation.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_xlm.txt\n",
      "File 'model_doc_xlm.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_audio-spectrogram-transformer.txt\n",
      "File 'model_doc_audio-spectrogram-transformer.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_llama.txt\n",
      "File 'model_doc_llama.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_regnet.txt\n",
      "File 'model_doc_regnet.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_document_question_answering.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_donut.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_poolformer.txt\n",
      "File 'model_doc_poolformer.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_cvt.txt\n",
      "File 'model_doc_cvt.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_dpr.txt\n",
      "File 'model_doc_dpr.txt' processed and saved to 'docs/prepared'.\n",
      "File 'internal_generation_utils.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_xlm-roberta-xl.txt\n",
      "File 'model_doc_xlm-roberta-xl.txt' processed and saved to 'docs/prepared'.\n",
      "File '_contributing.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_mms.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_question_answering.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_clip.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_superpoint.txt\n",
      "File 'model_doc_superpoint.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_mixtral.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_layoutlmv2.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_xlm-v.txt\n",
      "File 'model_doc_xlm-v.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_object_detection.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_bit.txt\n",
      "File 'model_doc_bit.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_monocular_depth_estimation.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_ijepa.txt\n",
      "File 'model_doc_ijepa.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_unispeech.txt\n",
      "File 'model_doc_unispeech.txt' processed and saved to 'docs/prepared'.\n",
      "File '_troubleshooting.txt' does not start with 'model'. Skipping...\n",
      "File '_index.txt' does not start with 'model'. Skipping...\n",
      "File '_llm_tutorial_optimization.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_seamless_m4t.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_mobilebert.txt\n",
      "File 'model_doc_mobilebert.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_kosmos-2.txt\n",
      "File 'model_doc_kosmos-2.txt' processed and saved to 'docs/prepared'.\n",
      "File 'main_classes_text_generation.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_layoutlmv3.txt\n",
      "File 'model_doc_layoutlmv3.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_xls_r.txt\n",
      "File 'model_doc_xls_r.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_xlnet.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_speech_to_text_2.txt\n",
      "File 'model_doc_speech_to_text_2.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_mobilevitv2.txt\n",
      "File 'model_doc_mobilevitv2.txt' processed and saved to 'docs/prepared'.\n",
      "File '_perf_infer_gpu_one.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_nllb.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_wav2vec2_phoneme.txt\n",
      "File 'model_doc_wav2vec2_phoneme.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_jamba.txt\n",
      "File 'model_doc_jamba.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_vitdet.txt\n",
      "File 'model_doc_vitdet.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_mt5.txt\n",
      "File 'model_doc_mt5.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_video_classification.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_codegen.txt\n",
      "File 'model_doc_codegen.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_efficientnet.txt\n",
      "File 'model_doc_efficientnet.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_wav2vec2-bert.txt\n",
      "File 'model_doc_wav2vec2-bert.txt' processed and saved to 'docs/prepared'.\n",
      "File 'quantization_bitsandbytes.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_keypoint_detection.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_beit.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_upernet.txt\n",
      "File 'model_doc_upernet.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_rag.txt\n",
      "File 'model_doc_rag.txt' processed and saved to 'docs/prepared'.\n",
      "File 'quantization_hqq.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_plbart.txt\n",
      "File 'model_doc_plbart.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_wav2vec2-conformer.txt\n",
      "File 'model_doc_wav2vec2-conformer.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_patchtst.txt\n",
      "File 'model_doc_patchtst.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_wavlm.txt\n",
      "File 'model_doc_wavlm.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_dpt.txt\n",
      "File 'model_doc_dpt.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_dbrx.txt\n",
      "File 'model_doc_dbrx.txt' processed and saved to 'docs/prepared'.\n",
      "File '_fsdp.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_bort.txt\n",
      "File 'model_doc_bort.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_gpt_neox.txt' does not start with 'model'. Skipping...\n",
      "File '_tflite.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_dit.txt\n",
      "File 'model_doc_dit.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_instructblipvideo.txt\n",
      "File 'model_doc_instructblipvideo.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_efficientformer.txt\n",
      "File 'model_doc_efficientformer.txt' processed and saved to 'docs/prepared'.\n",
      "File '__toctree.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_vitmatte.txt\n",
      "File 'model_doc_vitmatte.txt' processed and saved to 'docs/prepared'.\n",
      "File 'main_classes_optimizer_schedules.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_squeezebert.txt\n",
      "File 'model_doc_squeezebert.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_maskformer.txt\n",
      "File 'model_doc_maskformer.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_image_classification.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_glm.txt\n",
      "File 'model_doc_glm.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_image_feature_extraction.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_electra.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_nemotron.txt\n",
      "File 'model_doc_nemotron.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_falcon.txt\n",
      "File 'model_doc_falcon.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_vivit.txt\n",
      "File 'model_doc_vivit.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_mask2former.txt\n",
      "File 'model_doc_mask2former.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_nezha.txt\n",
      "File 'model_doc_nezha.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_grounding-dino.txt\n",
      "File 'model_doc_grounding-dino.txt' processed and saved to 'docs/prepared'.\n",
      "File '_modular_transformers.txt' does not start with 'model'. Skipping...\n",
      "File 'main_classes_keras_callbacks.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_mimi.txt\n",
      "File 'model_doc_mimi.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_yoso.txt\n",
      "File 'model_doc_yoso.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_imagegpt.txt\n",
      "File 'model_doc_imagegpt.txt' processed and saved to 'docs/prepared'.\n",
      "File '_tf_xla.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_persimmon.txt\n",
      "File 'model_doc_persimmon.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_olmo.txt\n",
      "File 'model_doc_olmo.txt' processed and saved to 'docs/prepared'.\n",
      "File '_deepspeed.txt' does not start with 'model'. Skipping...\n",
      "File 'main_classes_data_collator.txt' does not start with 'model'. Skipping...\n",
      "File '__redirects.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_realm.txt\n",
      "File 'model_doc_realm.txt' processed and saved to 'docs/prepared'.\n",
      "File '_pipeline_webserver.txt' does not start with 'model'. Skipping...\n",
      "File '_glossary.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_hiera.txt\n",
      "File 'model_doc_hiera.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_longformer.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_decision_transformer.txt\n",
      "File 'model_doc_decision_transformer.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_lilt.txt\n",
      "File 'model_doc_lilt.txt' processed and saved to 'docs/prepared'.\n",
      "File '_peft.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_vision-encoder-decoder.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_instructblip.txt\n",
      "File 'model_doc_instructblip.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_openai-gpt.txt\n",
      "File 'model_doc_openai-gpt.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_mega.txt\n",
      "File 'model_doc_mega.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_xglm.txt\n",
      "File 'model_doc_xglm.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_bigbird_pegasus.txt\n",
      "File 'model_doc_bigbird_pegasus.txt' processed and saved to 'docs/prepared'.\n",
      "File 'internal_trainer_utils.txt' does not start with 'model'. Skipping...\n",
      "File '_perf_train_gpu_many.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_gpt_bigcode.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_idefics.txt' does not start with 'model'. Skipping...\n",
      "File 'main_classes_processors.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_video_llava.txt' does not start with 'model'. Skipping...\n",
      "File '_add_new_model.txt' does not start with 'model'. Skipping...\n",
      "File '_kv_cache.txt' does not start with 'model'. Skipping...\n",
      "File '_training.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_ctrl.txt\n",
      "File 'model_doc_ctrl.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_olmoe.txt\n",
      "File 'model_doc_olmoe.txt' processed and saved to 'docs/prepared'.\n",
      "File '_pipeline_tutorial.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_olmo2.txt\n",
      "File 'model_doc_olmo2.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_zero_shot_image_classification.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_ernie.txt\n",
      "File 'model_doc_ernie.txt' processed and saved to 'docs/prepared'.\n",
      "File 'internal_audio_utils.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_table-transformer.txt\n",
      "File 'model_doc_table-transformer.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_bloom.txt\n",
      "File 'model_doc_bloom.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_patchtsmixer.txt\n",
      "File 'model_doc_patchtsmixer.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_vipllava.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_mbart.txt' does not start with 'model'. Skipping...\n",
      "File '_fast_tokenizers.txt' does not start with 'model'. Skipping...\n",
      "File '_perplexity.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_clvp.txt\n",
      "File 'model_doc_clvp.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_gptsan-japanese.txt\n",
      "File 'model_doc_gptsan-japanese.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_nllb-moe.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_seamless_m4t_v2.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_cpmant.txt\n",
      "File 'model_doc_cpmant.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_time_series_transformer.txt\n",
      "File 'model_doc_time_series_transformer.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_mistral.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_llava_onevision.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_nat.txt\n",
      "File 'model_doc_nat.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_autoformer.txt\n",
      "File 'model_doc_autoformer.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_esm.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_dinat.txt\n",
      "File 'model_doc_dinat.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_deberta.txt\n",
      "File 'model_doc_deberta.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_fuyu.txt\n",
      "File 'model_doc_fuyu.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_ul2.txt\n",
      "File 'model_doc_ul2.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_semantic_segmentation.txt' does not start with 'model'. Skipping...\n",
      "File 'tasks_multiple_choice.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_mobilenet_v1.txt\n",
      "File 'model_doc_mobilenet_v1.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_rembert.txt\n",
      "File 'model_doc_rembert.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_deplot.txt\n",
      "File 'model_doc_deplot.txt' processed and saved to 'docs/prepared'.\n",
      "File 'main_classes_backbones.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_code_llama.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_pvt_v2.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_aria.txt\n",
      "File 'model_doc_aria.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_starcoder2.txt\n",
      "File 'model_doc_starcoder2.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_phi.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_bert-generation.txt\n",
      "File 'model_doc_bert-generation.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_vit_mae.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_mra.txt\n",
      "File 'model_doc_mra.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_flan-ul2.txt\n",
      "File 'model_doc_flan-ul2.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_speech_to_text.txt\n",
      "File 'model_doc_speech_to_text.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_t5.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_gpt-sw3.txt\n",
      "File 'model_doc_gpt-sw3.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_moshi.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_vilt.txt\n",
      "File 'model_doc_vilt.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_gemma.txt\n",
      "File 'model_doc_gemma.txt' processed and saved to 'docs/prepared'.\n",
      "File 'quantization_bitnet.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_gptj.txt' does not start with 'model'. Skipping...\n",
      "File 'internal_pipelines_utils.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_roc_bert.txt\n",
      "File 'model_doc_roc_bert.txt' processed and saved to 'docs/prepared'.\n",
      "File '_run_scripts.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_swin2sr.txt\n",
      "File 'model_doc_swin2sr.txt' processed and saved to 'docs/prepared'.\n",
      "File '_pad_truncation.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_qwen2.txt\n",
      "File 'model_doc_qwen2.txt' processed and saved to 'docs/prepared'.\n",
      "File '_preprocessing.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_switch_transformers.txt\n",
      "File 'model_doc_switch_transformers.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_luke.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_cohere.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_auto.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_mobilenet_v2.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_mpnet.txt\n",
      "File 'model_doc_mpnet.txt' processed and saved to 'docs/prepared'.\n",
      "File 'tasks_mask_generation.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_phimoe.txt\n",
      "File 'model_doc_phimoe.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_vit_hybrid.txt\n",
      "File 'model_doc_vit_hybrid.txt' processed and saved to 'docs/prepared'.\n",
      "File '_how_to_hack_models.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_git.txt\n",
      "File 'model_doc_git.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_encodec.txt\n",
      "File 'model_doc_encodec.txt' processed and saved to 'docs/prepared'.\n",
      "Processing file: model_doc_fsmt.txt\n",
      "File 'model_doc_fsmt.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_deit.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_myt5.txt\n",
      "File 'model_doc_myt5.txt' processed and saved to 'docs/prepared'.\n",
      "File '_perf_train_special.txt' does not start with 'model'. Skipping...\n",
      "File 'model_doc_vits.txt' does not start with 'model'. Skipping...\n",
      "File 'main_classes_feature_extractor.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_tvlt.txt\n",
      "File 'model_doc_tvlt.txt' processed and saved to 'docs/prepared'.\n",
      "File 'quantization_fbgemm_fp8.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_depth_anything_v2.txt\n",
      "File 'model_doc_depth_anything_v2.txt' processed and saved to 'docs/prepared'.\n",
      "File 'main_classes_executorch.txt' does not start with 'model'. Skipping...\n",
      "Processing file: model_doc_megatron-bert.txt\n",
      "File 'model_doc_megatron-bert.txt' processed and saved to 'docs/prepared'.\n",
      "File 'model_doc_marian.txt' does not start with 'model'. Skipping...\n",
      "All files starting with 'model' have been processed and saved to 'docs/prepared'.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Document Embeddings",
   "id": "29ecc24259dfb476"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:50:15.468158Z",
     "start_time": "2024-12-11T22:41:10.558300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "RAG_API_URL = \"http://localhost:8000/add_to_rag_db\"\n",
    "prepared_dir = 'docs/prepared'\n",
    "\n",
    "def load_data_to_db():\n",
    "    for filename in os.listdir(prepared_dir):\n",
    "        file_path = os.path.join(prepared_dir, filename)\n",
    "\n",
    "        if os.path.isdir(file_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Read the file content\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "\n",
    "            # Split the content by the '\\n\\n' delimiter (double newlines)\n",
    "            chunks = content.split(\"\\n\\n\")\n",
    "\n",
    "            # Iterate over each chunk and send it to the API\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if chunk.strip():\n",
    "                    response = requests.post(RAG_API_URL, json={\"text\": chunk})\n",
    "                    if response.status_code == 200:\n",
    "                        print(f\"Chunk {i + 1}/{len(chunks)} of file '{filename}' successfully uploaded to the database\")\n",
    "                    else:\n",
    "                        print(f\"Error uploading chunk {i + 1}/{len(chunks)} of file '{filename}': {response.status_code} {response.text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing file '{filename}': {str(e)}\")\n",
    "            \n",
    "load_data_to_db()"
   ],
   "id": "abdb3e474b4dc5da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/8 of file 'model_doc_trocr.txt' successfully uploaded to the database\n",
      "Chunk 2/8 of file 'model_doc_trocr.txt' successfully uploaded to the database\n",
      "Chunk 3/8 of file 'model_doc_trocr.txt' successfully uploaded to the database\n",
      "Chunk 4/8 of file 'model_doc_trocr.txt' successfully uploaded to the database\n",
      "Chunk 5/8 of file 'model_doc_trocr.txt' successfully uploaded to the database\n",
      "Chunk 6/8 of file 'model_doc_trocr.txt' successfully uploaded to the database\n",
      "Chunk 7/8 of file 'model_doc_trocr.txt' successfully uploaded to the database\n",
      "Chunk 8/8 of file 'model_doc_trocr.txt' successfully uploaded to the database\n",
      "Chunk 1/3 of file 'model_doc_deberta-v2.txt' successfully uploaded to the database\n",
      "Chunk 2/3 of file 'model_doc_deberta-v2.txt' successfully uploaded to the database\n",
      "Chunk 3/3 of file 'model_doc_deberta-v2.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_longt5.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_longt5.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_longt5.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_longt5.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_longt5.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_longt5.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_gemma2.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_gemma2.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_gemma2.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_gemma2.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_xclip.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_xclip.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_xclip.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_xclip.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_xlm-prophetnet.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_xlm-prophetnet.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_xlm-prophetnet.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_xlm-prophetnet.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_xlm-prophetnet.txt' successfully uploaded to the database\n",
      "Chunk 1/7 of file 'model_doc_megatron_gpt2.txt' successfully uploaded to the database\n",
      "Chunk 2/7 of file 'model_doc_megatron_gpt2.txt' successfully uploaded to the database\n",
      "Chunk 3/7 of file 'model_doc_megatron_gpt2.txt' successfully uploaded to the database\n",
      "Chunk 4/7 of file 'model_doc_megatron_gpt2.txt' successfully uploaded to the database\n",
      "Chunk 5/7 of file 'model_doc_megatron_gpt2.txt' successfully uploaded to the database\n",
      "Chunk 6/7 of file 'model_doc_megatron_gpt2.txt' successfully uploaded to the database\n",
      "Chunk 7/7 of file 'model_doc_megatron_gpt2.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_blip.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_blip.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_blip.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_blip.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_convnextv2.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_convnextv2.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_convnextv2.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_convnextv2.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_convnextv2.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_open-llama.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_open-llama.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_open-llama.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_open-llama.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_llama3.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_llama3.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_llama3.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_llama3.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_llama3.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_llama3.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_layoutxlm.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_layoutxlm.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_layoutxlm.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_layoutxlm.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_seggpt.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_seggpt.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_seggpt.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_seggpt.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_seggpt.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_trajectory_transformer.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_trajectory_transformer.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_trajectory_transformer.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_trajectory_transformer.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_trajectory_transformer.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_pegasus_x.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_pegasus_x.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_pegasus_x.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_pegasus_x.txt' successfully uploaded to the database\n",
      "Chunk 1/7 of file 'model_doc_granitemoe.txt' successfully uploaded to the database\n",
      "Chunk 2/7 of file 'model_doc_granitemoe.txt' successfully uploaded to the database\n",
      "Chunk 3/7 of file 'model_doc_granitemoe.txt' successfully uploaded to the database\n",
      "Chunk 4/7 of file 'model_doc_granitemoe.txt' successfully uploaded to the database\n",
      "Chunk 5/7 of file 'model_doc_granitemoe.txt' successfully uploaded to the database\n",
      "Chunk 6/7 of file 'model_doc_granitemoe.txt' successfully uploaded to the database\n",
      "Chunk 7/7 of file 'model_doc_granitemoe.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_bert-japanese.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_bert-japanese.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_bert-japanese.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_bert-japanese.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_bert-japanese.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_nougat.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_nougat.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_nougat.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_nougat.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_nougat.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_nougat.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_granite.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_granite.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_granite.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_granite.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_granite.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_roberta-prelayernorm.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_roberta-prelayernorm.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_roberta-prelayernorm.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_roberta-prelayernorm.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_roberta-prelayernorm.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_informer.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_informer.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_informer.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_informer.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_clap.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_clap.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_clap.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_clap.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_gpt_neo.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_gpt_neo.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_gpt_neo.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_gpt_neo.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_gpt_neo.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_flava.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_flava.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_flava.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_flava.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_flava.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_univnet.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_univnet.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_univnet.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_univnet.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_swiftformer.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_swiftformer.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_swiftformer.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_swiftformer.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_swiftformer.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_glpn.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_glpn.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_glpn.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_glpn.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_glpn.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_hubert.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_hubert.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_hubert.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_hubert.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_hubert.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_hubert.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_xmod.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_xmod.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_xmod.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_xmod.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_xmod.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_xmod.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_timesformer.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_timesformer.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_timesformer.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_timesformer.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_timesformer.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_timesformer.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_clipseg.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_clipseg.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_clipseg.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_clipseg.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_clipseg.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_dinov2.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_dinov2.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_dinov2.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_dinov2.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_dinov2.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_barthez.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_barthez.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_barthez.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_barthez.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_barthez.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_madlad-400.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_madlad-400.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_madlad-400.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_madlad-400.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_madlad-400.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_madlad-400.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_ibert.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_ibert.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_ibert.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_ibert.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_ibert.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_big_bird.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_big_bird.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_big_bird.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_big_bird.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_big_bird.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_layoutlm.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_layoutlm.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_layoutlm.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_layoutlm.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_matcha.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_matcha.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_matcha.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_matcha.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_matcha.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_cpm.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_cpm.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_cpm.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_cpm.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_align.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_align.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_align.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_align.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_align.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_idefics.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_idefics.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_idefics.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_idefics.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_idefics.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_mctct.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_mctct.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_mctct.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_mctct.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_mctct.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_depth_anything.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_depth_anything.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_depth_anything.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_depth_anything.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_depth_anything.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_depth_anything.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_mgp-str.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_mgp-str.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_mgp-str.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_mgp-str.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_mgp-str.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_mgp-str.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_swin.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_swin.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_swin.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_swin.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_swin.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_swin.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_jetmoe.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_jetmoe.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_jetmoe.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_jetmoe.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_convbert.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_convbert.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_convbert.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_convbert.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_convbert.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_convbert.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_data2vec.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_data2vec.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_data2vec.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_data2vec.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_data2vec.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_mpt.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_mpt.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_mpt.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_mpt.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_herbert.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_herbert.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_herbert.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_herbert.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_herbert.txt' successfully uploaded to the database\n",
      "Chunk 1/2 of file 'model_doc_retribert.txt' successfully uploaded to the database\n",
      "Chunk 2/2 of file 'model_doc_retribert.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_mamba.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_mamba.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_mamba.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_mamba.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_mamba.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_mamba.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_dac.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_dac.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_dac.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_dac.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_dac.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_dac.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_xlsr_wav2vec2.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_xlsr_wav2vec2.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_xlsr_wav2vec2.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_xlsr_wav2vec2.txt' successfully uploaded to the database\n",
      "Chunk 1/7 of file 'model_doc_mllama.txt' successfully uploaded to the database\n",
      "Chunk 2/7 of file 'model_doc_mllama.txt' successfully uploaded to the database\n",
      "Chunk 3/7 of file 'model_doc_mllama.txt' successfully uploaded to the database\n",
      "Chunk 4/7 of file 'model_doc_mllama.txt' successfully uploaded to the database\n",
      "Chunk 5/7 of file 'model_doc_mllama.txt' successfully uploaded to the database\n",
      "Chunk 6/7 of file 'model_doc_mllama.txt' successfully uploaded to the database\n",
      "Chunk 7/7 of file 'model_doc_mllama.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_qwen2_moe.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_qwen2_moe.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_qwen2_moe.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_qwen2_moe.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_qwen2_moe.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_qwen2_moe.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_flan-t5.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_flan-t5.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_flan-t5.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_flan-t5.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_umt5.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_umt5.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_umt5.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_umt5.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_umt5.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_umt5.txt' successfully uploaded to the database\n",
      "Chunk 1/7 of file 'model_doc_groupvit.txt' successfully uploaded to the database\n",
      "Chunk 2/7 of file 'model_doc_groupvit.txt' successfully uploaded to the database\n",
      "Chunk 3/7 of file 'model_doc_groupvit.txt' successfully uploaded to the database\n",
      "Chunk 4/7 of file 'model_doc_groupvit.txt' successfully uploaded to the database\n",
      "Chunk 5/7 of file 'model_doc_groupvit.txt' successfully uploaded to the database\n",
      "Chunk 6/7 of file 'model_doc_groupvit.txt' successfully uploaded to the database\n",
      "Chunk 7/7 of file 'model_doc_groupvit.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_paligemma.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_paligemma.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_paligemma.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_paligemma.txt' successfully uploaded to the database\n",
      "Chunk 1/9 of file 'model_doc_stablelm.txt' successfully uploaded to the database\n",
      "Chunk 2/9 of file 'model_doc_stablelm.txt' successfully uploaded to the database\n",
      "Chunk 3/9 of file 'model_doc_stablelm.txt' successfully uploaded to the database\n",
      "Chunk 4/9 of file 'model_doc_stablelm.txt' successfully uploaded to the database\n",
      "Chunk 5/9 of file 'model_doc_stablelm.txt' successfully uploaded to the database\n",
      "Chunk 6/9 of file 'model_doc_stablelm.txt' successfully uploaded to the database\n",
      "Chunk 7/9 of file 'model_doc_stablelm.txt' successfully uploaded to the database\n",
      "Chunk 8/9 of file 'model_doc_stablelm.txt' successfully uploaded to the database\n",
      "Chunk 9/9 of file 'model_doc_stablelm.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_falcon_mamba.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_falcon_mamba.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_falcon_mamba.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_falcon_mamba.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_blip-2.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_blip-2.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_blip-2.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_blip-2.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_blip-2.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_pvt.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_pvt.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_pvt.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_pvt.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_led.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_led.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_led.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_led.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_led.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_videomae.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_videomae.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_videomae.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_videomae.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_videomae.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_videomae.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_ernie_m.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_ernie_m.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_ernie_m.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_ernie_m.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_ernie_m.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_prophetnet.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_prophetnet.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_prophetnet.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_prophetnet.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_prophetnet.txt' successfully uploaded to the database\n",
      "Chunk 1/7 of file 'model_doc_graphormer.txt' successfully uploaded to the database\n",
      "Chunk 2/7 of file 'model_doc_graphormer.txt' successfully uploaded to the database\n",
      "Chunk 3/7 of file 'model_doc_graphormer.txt' successfully uploaded to the database\n",
      "Chunk 4/7 of file 'model_doc_graphormer.txt' successfully uploaded to the database\n",
      "Chunk 5/7 of file 'model_doc_graphormer.txt' successfully uploaded to the database\n",
      "Chunk 6/7 of file 'model_doc_graphormer.txt' successfully uploaded to the database\n",
      "Chunk 7/7 of file 'model_doc_graphormer.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_sew.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_sew.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_sew.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_sew.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_deformable_detr.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_deformable_detr.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_deformable_detr.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_deformable_detr.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_camembert.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_camembert.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_camembert.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_camembert.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_funnel.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_funnel.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_funnel.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_funnel.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_funnel.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_pix2struct.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_pix2struct.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_pix2struct.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_pix2struct.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_bartpho.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_bartpho.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_bartpho.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_bartpho.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_bartpho.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_van.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_van.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_van.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_van.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_van.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_van.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_flaubert.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_flaubert.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_flaubert.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_flaubert.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_flaubert.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_oneformer.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_oneformer.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_oneformer.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_oneformer.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_oneformer.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_oneformer.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_vit_msn.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_vit_msn.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_vit_msn.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_vit_msn.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_vit_msn.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_vit_msn.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_fnet.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_fnet.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_fnet.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_fnet.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_swinv2.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_swinv2.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_swinv2.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_swinv2.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_swinv2.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_blenderbot.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_blenderbot.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_blenderbot.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_blenderbot.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_blenderbot.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_blenderbot.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_dialogpt.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_dialogpt.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_dialogpt.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_dialogpt.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_dialogpt.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_bertweet.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_bertweet.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_bertweet.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_bertweet.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_bertweet.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_rt_detr.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_rt_detr.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_rt_detr.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_rt_detr.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_rt_detr.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_zamba.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_zamba.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_zamba.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_zamba.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_zamba.txt' successfully uploaded to the database\n",
      "Chunk 1/7 of file 'model_doc_mobilevit.txt' successfully uploaded to the database\n",
      "Chunk 2/7 of file 'model_doc_mobilevit.txt' successfully uploaded to the database\n",
      "Chunk 3/7 of file 'model_doc_mobilevit.txt' successfully uploaded to the database\n",
      "Chunk 4/7 of file 'model_doc_mobilevit.txt' successfully uploaded to the database\n",
      "Chunk 5/7 of file 'model_doc_mobilevit.txt' successfully uploaded to the database\n",
      "Chunk 6/7 of file 'model_doc_mobilevit.txt' successfully uploaded to the database\n",
      "Chunk 7/7 of file 'model_doc_mobilevit.txt' successfully uploaded to the database\n",
      "Chunk 1/7 of file 'model_doc_altclip.txt' successfully uploaded to the database\n",
      "Chunk 2/7 of file 'model_doc_altclip.txt' successfully uploaded to the database\n",
      "Chunk 3/7 of file 'model_doc_altclip.txt' successfully uploaded to the database\n",
      "Chunk 4/7 of file 'model_doc_altclip.txt' successfully uploaded to the database\n",
      "Chunk 5/7 of file 'model_doc_altclip.txt' successfully uploaded to the database\n",
      "Chunk 6/7 of file 'model_doc_altclip.txt' successfully uploaded to the database\n",
      "Chunk 7/7 of file 'model_doc_altclip.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_deta.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_deta.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_deta.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_deta.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_deta.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_deta.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_jukebox.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_jukebox.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_jukebox.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_jukebox.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_jukebox.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_canine.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_canine.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_canine.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_canine.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_canine.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_canine.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_mluke.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_mluke.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_mluke.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_mluke.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_mluke.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_speecht5.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_speecht5.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_speecht5.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_speecht5.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_roformer.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_roformer.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_roformer.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_roformer.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_roformer.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_pegasus.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_pegasus.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_pegasus.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_pegasus.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_pegasus.txt' successfully uploaded to the database\n",
      "Chunk 1/3 of file 'model_doc_nystromformer.txt' successfully uploaded to the database\n",
      "Chunk 2/3 of file 'model_doc_nystromformer.txt' successfully uploaded to the database\n",
      "Chunk 3/3 of file 'model_doc_nystromformer.txt' successfully uploaded to the database\n",
      "Chunk 1/7 of file 'model_doc_focalnet.txt' successfully uploaded to the database\n",
      "Chunk 2/7 of file 'model_doc_focalnet.txt' successfully uploaded to the database\n",
      "Chunk 3/7 of file 'model_doc_focalnet.txt' successfully uploaded to the database\n",
      "Chunk 4/7 of file 'model_doc_focalnet.txt' successfully uploaded to the database\n",
      "Chunk 5/7 of file 'model_doc_focalnet.txt' successfully uploaded to the database\n",
      "Chunk 6/7 of file 'model_doc_focalnet.txt' successfully uploaded to the database\n",
      "Chunk 7/7 of file 'model_doc_focalnet.txt' successfully uploaded to the database\n",
      "Chunk 1/3 of file 'model_doc_vision-text-dual-encoder.txt' successfully uploaded to the database\n",
      "Chunk 2/3 of file 'model_doc_vision-text-dual-encoder.txt' successfully uploaded to the database\n",
      "Chunk 3/3 of file 'model_doc_vision-text-dual-encoder.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_unispeech-sat.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_unispeech-sat.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_unispeech-sat.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_unispeech-sat.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_unispeech-sat.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_mamba2.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_mamba2.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_mamba2.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_mamba2.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_mamba2.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_mamba2.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_pixtral.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_pixtral.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_pixtral.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_pixtral.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_pixtral.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_splinter.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_splinter.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_splinter.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_splinter.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_splinter.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_splinter.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_sew-d.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_sew-d.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_sew-d.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_sew-d.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_phobert.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_phobert.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_phobert.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_phobert.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_phobert.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_yolos.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_yolos.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_yolos.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_yolos.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_yolos.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_yolos.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_chinese_clip.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_chinese_clip.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_chinese_clip.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_chinese_clip.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_chinese_clip.txt' successfully uploaded to the database\n",
      "Chunk 1/3 of file 'model_doc_recurrent_gemma.txt' successfully uploaded to the database\n",
      "Chunk 2/3 of file 'model_doc_recurrent_gemma.txt' successfully uploaded to the database\n",
      "Chunk 3/3 of file 'model_doc_recurrent_gemma.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_idefics3.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_idefics3.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_idefics3.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_idefics3.txt' successfully uploaded to the database\n",
      "Chunk 1/7 of file 'model_doc_gpt_neox_japanese.txt' successfully uploaded to the database\n",
      "Chunk 2/7 of file 'model_doc_gpt_neox_japanese.txt' successfully uploaded to the database\n",
      "Chunk 3/7 of file 'model_doc_gpt_neox_japanese.txt' successfully uploaded to the database\n",
      "Chunk 4/7 of file 'model_doc_gpt_neox_japanese.txt' successfully uploaded to the database\n",
      "Chunk 5/7 of file 'model_doc_gpt_neox_japanese.txt' successfully uploaded to the database\n",
      "Chunk 6/7 of file 'model_doc_gpt_neox_japanese.txt' successfully uploaded to the database\n",
      "Chunk 7/7 of file 'model_doc_gpt_neox_japanese.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_phi3.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_phi3.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_phi3.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_phi3.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_phi3.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_convnext.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_convnext.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_convnext.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_convnext.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_convnext.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_blenderbot-small.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_blenderbot-small.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_blenderbot-small.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_blenderbot-small.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_blenderbot-small.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_visual_bert.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_visual_bert.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_visual_bert.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_visual_bert.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_visual_bert.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_visual_bert.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_lxmert.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_lxmert.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_lxmert.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_lxmert.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_lxmert.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_conditional_detr.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_conditional_detr.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_conditional_detr.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_conditional_detr.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_resnet.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_resnet.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_resnet.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_resnet.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_resnet.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_resnet.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_t5v1.1.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_t5v1.1.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_t5v1.1.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_t5v1.1.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_t5v1.1.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_t5v1.1.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_xlm.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_xlm.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_xlm.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_xlm.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_xlm.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_xlm.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_audio-spectrogram-transformer.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_audio-spectrogram-transformer.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_audio-spectrogram-transformer.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_audio-spectrogram-transformer.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_audio-spectrogram-transformer.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_audio-spectrogram-transformer.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_llama.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_llama.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_llama.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_llama.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_llama.txt' successfully uploaded to the database\n",
      "Chunk 1/7 of file 'model_doc_regnet.txt' successfully uploaded to the database\n",
      "Chunk 2/7 of file 'model_doc_regnet.txt' successfully uploaded to the database\n",
      "Chunk 3/7 of file 'model_doc_regnet.txt' successfully uploaded to the database\n",
      "Chunk 4/7 of file 'model_doc_regnet.txt' successfully uploaded to the database\n",
      "Chunk 5/7 of file 'model_doc_regnet.txt' successfully uploaded to the database\n",
      "Chunk 6/7 of file 'model_doc_regnet.txt' successfully uploaded to the database\n",
      "Chunk 7/7 of file 'model_doc_regnet.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_poolformer.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_poolformer.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_poolformer.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_poolformer.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_poolformer.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_poolformer.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_cvt.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_cvt.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_cvt.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_cvt.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_cvt.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_dpr.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_dpr.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_dpr.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_dpr.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_xlm-roberta-xl.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_xlm-roberta-xl.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_xlm-roberta-xl.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_xlm-roberta-xl.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_superpoint.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_superpoint.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_superpoint.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_superpoint.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_superpoint.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_superpoint.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_xlm-v.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_xlm-v.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_xlm-v.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_xlm-v.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_xlm-v.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_xlm-v.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_bit.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_bit.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_bit.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_bit.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_ijepa.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_ijepa.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_ijepa.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_ijepa.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_ijepa.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_unispeech.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_unispeech.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_unispeech.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_unispeech.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_mobilebert.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_mobilebert.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_mobilebert.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_mobilebert.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_mobilebert.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_kosmos-2.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_kosmos-2.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_kosmos-2.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_kosmos-2.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_kosmos-2.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_layoutlmv3.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_layoutlmv3.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_layoutlmv3.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_layoutlmv3.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_layoutlmv3.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_xls_r.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_xls_r.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_xls_r.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_xls_r.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_xls_r.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_xls_r.txt' successfully uploaded to the database\n",
      "Chunk 1/7 of file 'model_doc_speech_to_text_2.txt' successfully uploaded to the database\n",
      "Chunk 2/7 of file 'model_doc_speech_to_text_2.txt' successfully uploaded to the database\n",
      "Chunk 3/7 of file 'model_doc_speech_to_text_2.txt' successfully uploaded to the database\n",
      "Chunk 4/7 of file 'model_doc_speech_to_text_2.txt' successfully uploaded to the database\n",
      "Chunk 5/7 of file 'model_doc_speech_to_text_2.txt' successfully uploaded to the database\n",
      "Chunk 6/7 of file 'model_doc_speech_to_text_2.txt' successfully uploaded to the database\n",
      "Chunk 7/7 of file 'model_doc_speech_to_text_2.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_mobilevitv2.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_mobilevitv2.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_mobilevitv2.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_mobilevitv2.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_mobilevitv2.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_wav2vec2_phoneme.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_wav2vec2_phoneme.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_wav2vec2_phoneme.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_wav2vec2_phoneme.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_wav2vec2_phoneme.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_wav2vec2_phoneme.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_jamba.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_jamba.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_jamba.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_jamba.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_jamba.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_jamba.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_vitdet.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_vitdet.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_vitdet.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_vitdet.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_mt5.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_mt5.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_mt5.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_mt5.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_codegen.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_codegen.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_codegen.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_codegen.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_codegen.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_codegen.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_efficientnet.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_efficientnet.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_efficientnet.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_efficientnet.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_wav2vec2-bert.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_wav2vec2-bert.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_wav2vec2-bert.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_wav2vec2-bert.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_wav2vec2-bert.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_wav2vec2-bert.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_upernet.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_upernet.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_upernet.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_upernet.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_rag.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_rag.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_rag.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_rag.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_rag.txt' successfully uploaded to the database\n",
      "Chunk 1/7 of file 'model_doc_plbart.txt' successfully uploaded to the database\n",
      "Chunk 2/7 of file 'model_doc_plbart.txt' successfully uploaded to the database\n",
      "Chunk 3/7 of file 'model_doc_plbart.txt' successfully uploaded to the database\n",
      "Chunk 4/7 of file 'model_doc_plbart.txt' successfully uploaded to the database\n",
      "Chunk 5/7 of file 'model_doc_plbart.txt' successfully uploaded to the database\n",
      "Chunk 6/7 of file 'model_doc_plbart.txt' successfully uploaded to the database\n",
      "Chunk 7/7 of file 'model_doc_plbart.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_wav2vec2-conformer.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_wav2vec2-conformer.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_wav2vec2-conformer.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_wav2vec2-conformer.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_patchtst.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_patchtst.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_patchtst.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_patchtst.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_patchtst.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_wavlm.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_wavlm.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_wavlm.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_wavlm.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_wavlm.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_wavlm.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_dpt.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_dpt.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_dpt.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_dpt.txt' successfully uploaded to the database\n",
      "Chunk 1/7 of file 'model_doc_dbrx.txt' successfully uploaded to the database\n",
      "Chunk 2/7 of file 'model_doc_dbrx.txt' successfully uploaded to the database\n",
      "Chunk 3/7 of file 'model_doc_dbrx.txt' successfully uploaded to the database\n",
      "Chunk 4/7 of file 'model_doc_dbrx.txt' successfully uploaded to the database\n",
      "Chunk 5/7 of file 'model_doc_dbrx.txt' successfully uploaded to the database\n",
      "Chunk 6/7 of file 'model_doc_dbrx.txt' successfully uploaded to the database\n",
      "Chunk 7/7 of file 'model_doc_dbrx.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_bort.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_bort.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_bort.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_bort.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_dit.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_dit.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_dit.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_dit.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_dit.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_instructblipvideo.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_instructblipvideo.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_instructblipvideo.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_instructblipvideo.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_instructblipvideo.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_efficientformer.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_efficientformer.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_efficientformer.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_efficientformer.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_efficientformer.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_efficientformer.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_vitmatte.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_vitmatte.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_vitmatte.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_vitmatte.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_vitmatte.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_vitmatte.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_squeezebert.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_squeezebert.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_squeezebert.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_squeezebert.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_squeezebert.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_maskformer.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_maskformer.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_maskformer.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_maskformer.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_maskformer.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_maskformer.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_glm.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_glm.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_glm.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_glm.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_nemotron.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_nemotron.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_nemotron.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_nemotron.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_nemotron.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_falcon.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_falcon.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_falcon.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_falcon.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_falcon.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_vivit.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_vivit.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_vivit.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_vivit.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_vivit.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_mask2former.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_mask2former.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_mask2former.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_mask2former.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_mask2former.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_nezha.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_nezha.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_nezha.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_nezha.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_nezha.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_grounding-dino.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_grounding-dino.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_grounding-dino.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_grounding-dino.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_grounding-dino.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_grounding-dino.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_mimi.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_mimi.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_mimi.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_mimi.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_mimi.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_mimi.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_yoso.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_yoso.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_yoso.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_yoso.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_yoso.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_yoso.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_imagegpt.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_imagegpt.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_imagegpt.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_imagegpt.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_imagegpt.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_persimmon.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_persimmon.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_persimmon.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_persimmon.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_persimmon.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_persimmon.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_olmo.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_olmo.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_olmo.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_olmo.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_realm.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_realm.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_realm.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_realm.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_realm.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_hiera.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_hiera.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_hiera.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_hiera.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_decision_transformer.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_decision_transformer.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_decision_transformer.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_decision_transformer.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_lilt.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_lilt.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_lilt.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_lilt.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_instructblip.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_instructblip.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_instructblip.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_instructblip.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_instructblip.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_openai-gpt.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_openai-gpt.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_openai-gpt.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_openai-gpt.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_openai-gpt.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_openai-gpt.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_mega.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_mega.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_mega.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_mega.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_mega.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_mega.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_xglm.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_xglm.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_xglm.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_xglm.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_bigbird_pegasus.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_bigbird_pegasus.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_bigbird_pegasus.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_bigbird_pegasus.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_bigbird_pegasus.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_ctrl.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_ctrl.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_ctrl.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_ctrl.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_ctrl.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_ctrl.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_olmoe.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_olmoe.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_olmoe.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_olmoe.txt' successfully uploaded to the database\n",
      "Chunk 1/3 of file 'model_doc_olmo2.txt' successfully uploaded to the database\n",
      "Chunk 2/3 of file 'model_doc_olmo2.txt' successfully uploaded to the database\n",
      "Chunk 3/3 of file 'model_doc_olmo2.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_ernie.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_ernie.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_ernie.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_ernie.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_ernie.txt' successfully uploaded to the database\n",
      "Chunk 1/3 of file 'model_doc_table-transformer.txt' successfully uploaded to the database\n",
      "Chunk 2/3 of file 'model_doc_table-transformer.txt' successfully uploaded to the database\n",
      "Chunk 3/3 of file 'model_doc_table-transformer.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_bloom.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_bloom.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_bloom.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_bloom.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_bloom.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_patchtsmixer.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_patchtsmixer.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_patchtsmixer.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_patchtsmixer.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_patchtsmixer.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_clvp.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_clvp.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_clvp.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_clvp.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_clvp.txt' successfully uploaded to the database\n",
      "Chunk 1/8 of file 'model_doc_gptsan-japanese.txt' successfully uploaded to the database\n",
      "Chunk 2/8 of file 'model_doc_gptsan-japanese.txt' successfully uploaded to the database\n",
      "Chunk 3/8 of file 'model_doc_gptsan-japanese.txt' successfully uploaded to the database\n",
      "Chunk 4/8 of file 'model_doc_gptsan-japanese.txt' successfully uploaded to the database\n",
      "Chunk 5/8 of file 'model_doc_gptsan-japanese.txt' successfully uploaded to the database\n",
      "Chunk 6/8 of file 'model_doc_gptsan-japanese.txt' successfully uploaded to the database\n",
      "Chunk 7/8 of file 'model_doc_gptsan-japanese.txt' successfully uploaded to the database\n",
      "Chunk 8/8 of file 'model_doc_gptsan-japanese.txt' successfully uploaded to the database\n",
      "Chunk 1/3 of file 'model_doc_cpmant.txt' successfully uploaded to the database\n",
      "Chunk 2/3 of file 'model_doc_cpmant.txt' successfully uploaded to the database\n",
      "Chunk 3/3 of file 'model_doc_cpmant.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_time_series_transformer.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_time_series_transformer.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_time_series_transformer.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_time_series_transformer.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_time_series_transformer.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_nat.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_nat.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_nat.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_nat.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_nat.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_nat.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_autoformer.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_autoformer.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_autoformer.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_autoformer.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_autoformer.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_autoformer.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_dinat.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_dinat.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_dinat.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_dinat.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_dinat.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_dinat.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_deberta.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_deberta.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_deberta.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_deberta.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_deberta.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_fuyu.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_fuyu.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_fuyu.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_fuyu.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_fuyu.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_fuyu.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_ul2.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_ul2.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_ul2.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_ul2.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_ul2.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_mobilenet_v1.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_mobilenet_v1.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_mobilenet_v1.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_mobilenet_v1.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_mobilenet_v1.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_mobilenet_v1.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_rembert.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_rembert.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_rembert.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_rembert.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_rembert.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_rembert.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_deplot.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_deplot.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_deplot.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_deplot.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_deplot.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_aria.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_aria.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_aria.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_aria.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_starcoder2.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_starcoder2.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_starcoder2.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_starcoder2.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_bert-generation.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_bert-generation.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_bert-generation.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_bert-generation.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_bert-generation.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_mra.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_mra.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_mra.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_mra.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_flan-ul2.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_flan-ul2.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_flan-ul2.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_flan-ul2.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_flan-ul2.txt' successfully uploaded to the database\n",
      "Chunk 1/7 of file 'model_doc_speech_to_text.txt' successfully uploaded to the database\n",
      "Chunk 2/7 of file 'model_doc_speech_to_text.txt' successfully uploaded to the database\n",
      "Chunk 3/7 of file 'model_doc_speech_to_text.txt' successfully uploaded to the database\n",
      "Chunk 4/7 of file 'model_doc_speech_to_text.txt' successfully uploaded to the database\n",
      "Chunk 5/7 of file 'model_doc_speech_to_text.txt' successfully uploaded to the database\n",
      "Chunk 6/7 of file 'model_doc_speech_to_text.txt' successfully uploaded to the database\n",
      "Chunk 7/7 of file 'model_doc_speech_to_text.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_gpt-sw3.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_gpt-sw3.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_gpt-sw3.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_gpt-sw3.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_gpt-sw3.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_gpt-sw3.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_vilt.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_vilt.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_vilt.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_vilt.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_gemma.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_gemma.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_gemma.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_gemma.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_gemma.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_roc_bert.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_roc_bert.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_roc_bert.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_roc_bert.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_roc_bert.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_swin2sr.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_swin2sr.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_swin2sr.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_swin2sr.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_swin2sr.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_qwen2.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_qwen2.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_qwen2.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_qwen2.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_switch_transformers.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_switch_transformers.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_switch_transformers.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_switch_transformers.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_switch_transformers.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_switch_transformers.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_mpnet.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_mpnet.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_mpnet.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_mpnet.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_mpnet.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_mpnet.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_phimoe.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_phimoe.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_phimoe.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_phimoe.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_phimoe.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_phimoe.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_vit_hybrid.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_vit_hybrid.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_vit_hybrid.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_vit_hybrid.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_vit_hybrid.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_vit_hybrid.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_git.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_git.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_git.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_git.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_encodec.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_encodec.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_encodec.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_encodec.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_encodec.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_encodec.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_fsmt.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_fsmt.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_fsmt.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_fsmt.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_fsmt.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_myt5.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_myt5.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_myt5.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_myt5.txt' successfully uploaded to the database\n",
      "Chunk 1/5 of file 'model_doc_tvlt.txt' successfully uploaded to the database\n",
      "Chunk 2/5 of file 'model_doc_tvlt.txt' successfully uploaded to the database\n",
      "Chunk 3/5 of file 'model_doc_tvlt.txt' successfully uploaded to the database\n",
      "Chunk 4/5 of file 'model_doc_tvlt.txt' successfully uploaded to the database\n",
      "Chunk 5/5 of file 'model_doc_tvlt.txt' successfully uploaded to the database\n",
      "Chunk 1/6 of file 'model_doc_depth_anything_v2.txt' successfully uploaded to the database\n",
      "Chunk 2/6 of file 'model_doc_depth_anything_v2.txt' successfully uploaded to the database\n",
      "Chunk 3/6 of file 'model_doc_depth_anything_v2.txt' successfully uploaded to the database\n",
      "Chunk 4/6 of file 'model_doc_depth_anything_v2.txt' successfully uploaded to the database\n",
      "Chunk 5/6 of file 'model_doc_depth_anything_v2.txt' successfully uploaded to the database\n",
      "Chunk 6/6 of file 'model_doc_depth_anything_v2.txt' successfully uploaded to the database\n",
      "Chunk 1/4 of file 'model_doc_megatron-bert.txt' successfully uploaded to the database\n",
      "Chunk 2/4 of file 'model_doc_megatron-bert.txt' successfully uploaded to the database\n",
      "Chunk 3/4 of file 'model_doc_megatron-bert.txt' successfully uploaded to the database\n",
      "Chunk 4/4 of file 'model_doc_megatron-bert.txt' successfully uploaded to the database\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Reranker Implementation",
   "id": "1a6e559dc673310e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T11:03:55.345408Z",
     "start_time": "2024-12-12T10:03:04.468108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "\n",
    "# Prompts derived from the provided texts\n",
    "prompts = [\n",
    "    \"What are the primary NLP tasks where BERTweet outperforms RoBERTa and XLM-R?\",\n",
    "    \"How does BERTweet differ from standard BERT in its implementation and tokenization methods?\",\n",
    "    \"How does BigBird's sparse attention mechanism achieve linear complexity for processing long sequences?\",\n",
    "    \"What potential applications does BigBird have in genomics data analysis?\",\n",
    "    \"Explain how CANINE processes text directly at the Unicode character level without tokenization.\",\n",
    "    \"How does CANINE compare to mBERT in terms of language adaptation and parameter efficiency?\",\n",
    "    \"What are the key differences between CPM and GPT-2, particularly regarding Chinese NLP tasks?\",\n",
    "    \"How does CPM excel in few-shot learning for Chinese language applications?\",\n",
    "    \"What advantages does JetMoe-8B's sparsely activated architecture offer over dense language models?\",\n",
    "    \"Describe the function of Mixture of Attention Heads and Mixture of MLP Experts in JetMoe-8B.\"\n",
    "]\n",
    "\n",
    "RAG_CONTEXT_API_URL = 'http://localhost:8000/prompt_w_context'\n",
    "reranker_dir = 'docs/reranked'\n",
    "\n",
    "class RAGRequest(BaseModel):\n",
    "    query: str\n",
    "    model: str = \"gpt-4o-mini\"\n",
    "    use_reranker: bool = False\n",
    "    top_k_retrieve: int = 10\n",
    "    top_k_rank: int = 1\n",
    "\n",
    "class RAGResponse(BaseModel):\n",
    "    message: str\n",
    "    context: list[str]\n",
    "    \n",
    "    \n",
    "# send request to the API and measure response time\n",
    "def send_rag_request(query, use_reranker=False, top_k_retrieve=10, top_k_rank=1):\n",
    "    url = RAG_CONTEXT_API_URL\n",
    "    payload = RAGRequest(\n",
    "        query=query,\n",
    "        use_reranker=use_reranker,\n",
    "        top_k_retrieve=top_k_retrieve,\n",
    "        top_k_rank=top_k_rank\n",
    "    ).model_dump_json()\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, data=payload, headers=headers)\n",
    "    response_time = time.time() - start_time\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        response_data = RAGResponse.model_validate_json(response.text)  # Updated method\n",
    "        return response_data, response_time\n",
    "    else:\n",
    "        return None, response_time\n",
    "\n",
    "\n",
    "def run_comparisons(prompts, top_k_values=[1, 3, 5], top_k_retrieve=10):\n",
    "    results = []\n",
    "    \n",
    "    print(\"Starting comparisons...\")\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"Processing prompt: '{prompt}'\")\n",
    "        \n",
    "        for top_k_rank in top_k_values:\n",
    "            print(f\"  Top-k rank: {top_k_rank}\")\n",
    "\n",
    "            print(f\"    Sending request without reranker (top_k_rank={top_k_rank})\")\n",
    "            context_without_reranker, time_without_reranker = send_rag_request(\n",
    "                query=prompt,\n",
    "                use_reranker=False,\n",
    "                top_k_retrieve=top_k_retrieve,\n",
    "                top_k_rank=top_k_rank\n",
    "            )\n",
    "            if context_without_reranker:\n",
    "                print(f\"    Context retrieved without reranker: {len(context_without_reranker.context)} documents\")\n",
    "            else:\n",
    "                print(f\"    No context retrieved without reranker for prompt '{prompt}'\")\n",
    "\n",
    "            print(f\"    Sending request with reranker (top_k_rank={top_k_rank})\")\n",
    "            context_with_reranker, time_with_reranker = send_rag_request(\n",
    "                query=prompt,\n",
    "                use_reranker=True,\n",
    "                top_k_retrieve=top_k_retrieve,\n",
    "                top_k_rank=top_k_rank\n",
    "            )\n",
    "            if context_with_reranker:\n",
    "                print(f\"    Context retrieved with reranker: {len(context_with_reranker.context)} documents\")\n",
    "            else:\n",
    "                print(f\"    No context retrieved with reranker for prompt '{prompt}'\")\n",
    "            \n",
    "            print(f\"    Time without reranker: {time_without_reranker:.2f}s\")\n",
    "            print(f\"    Time with reranker: {time_with_reranker:.2f}s\")\n",
    "            \n",
    "            results.append({\n",
    "                'query': prompt,\n",
    "                'top_k_rank': top_k_rank,\n",
    "                'context_without_reranker': context_without_reranker.context if context_without_reranker else [],\n",
    "                'context_with_reranker': context_with_reranker.context if context_with_reranker else [],\n",
    "                'time_without_reranker': time_without_reranker,\n",
    "                'time_with_reranker': time_with_reranker\n",
    "            })\n",
    "\n",
    "    print(\"Completed all comparisons.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results(results, filename=\"rag_comparisons.json\"):\n",
    "    if not os.path.exists(reranker_dir):\n",
    "        os.makedirs(reranker_dir)\n",
    "        \n",
    "    result_file = os.path.join(reranker_dir, filename)\n",
    "    \n",
    "    with open(result_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "            \n",
    "    print(f\"File '{filename}' results saved to '{reranker_dir}'.\")\n",
    "    \n",
    "# Run the comparison and save results\n",
    "results = run_comparisons(prompts)\n",
    "save_results(results)"
   ],
   "id": "9b3886bf29b60d27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comparisons...\n",
      "Processing prompt: 'What are the primary NLP tasks where BERTweet outperforms RoBERTa and XLM-R?'\n",
      "  Top-k rank: 1\n",
      "    Sending request without reranker (top_k_rank=1)\n",
      "    Context retrieved without reranker: 1 documents\n",
      "    Sending request with reranker (top_k_rank=1)\n",
      "    Context retrieved with reranker: 1 documents\n",
      "    Time without reranker: 2.04s\n",
      "    Time with reranker: 125.90s\n",
      "  Top-k rank: 3\n",
      "    Sending request without reranker (top_k_rank=3)\n",
      "    Context retrieved without reranker: 3 documents\n",
      "    Sending request with reranker (top_k_rank=3)\n",
      "    Context retrieved with reranker: 3 documents\n",
      "    Time without reranker: 1.97s\n",
      "    Time with reranker: 126.83s\n",
      "  Top-k rank: 5\n",
      "    Sending request without reranker (top_k_rank=5)\n",
      "    Context retrieved without reranker: 5 documents\n",
      "    Sending request with reranker (top_k_rank=5)\n",
      "    Context retrieved with reranker: 5 documents\n",
      "    Time without reranker: 1.97s\n",
      "    Time with reranker: 129.25s\n",
      "Processing prompt: 'How does BERTweet differ from standard BERT in its implementation and tokenization methods?'\n",
      "  Top-k rank: 1\n",
      "    Sending request without reranker (top_k_rank=1)\n",
      "    Context retrieved without reranker: 1 documents\n",
      "    Sending request with reranker (top_k_rank=1)\n",
      "    Context retrieved with reranker: 1 documents\n",
      "    Time without reranker: 1.74s\n",
      "    Time with reranker: 110.98s\n",
      "  Top-k rank: 3\n",
      "    Sending request without reranker (top_k_rank=3)\n",
      "    Context retrieved without reranker: 3 documents\n",
      "    Sending request with reranker (top_k_rank=3)\n",
      "    Context retrieved with reranker: 3 documents\n",
      "    Time without reranker: 1.67s\n",
      "    Time with reranker: 114.52s\n",
      "  Top-k rank: 5\n",
      "    Sending request without reranker (top_k_rank=5)\n",
      "    Context retrieved without reranker: 5 documents\n",
      "    Sending request with reranker (top_k_rank=5)\n",
      "    Context retrieved with reranker: 5 documents\n",
      "    Time without reranker: 1.68s\n",
      "    Time with reranker: 118.05s\n",
      "Processing prompt: 'How does BigBird's sparse attention mechanism achieve linear complexity for processing long sequences?'\n",
      "  Top-k rank: 1\n",
      "    Sending request without reranker (top_k_rank=1)\n",
      "    Context retrieved without reranker: 1 documents\n",
      "    Sending request with reranker (top_k_rank=1)\n",
      "    Context retrieved with reranker: 1 documents\n",
      "    Time without reranker: 1.67s\n",
      "    Time with reranker: 128.22s\n",
      "  Top-k rank: 3\n",
      "    Sending request without reranker (top_k_rank=3)\n",
      "    Context retrieved without reranker: 3 documents\n",
      "    Sending request with reranker (top_k_rank=3)\n",
      "    Context retrieved with reranker: 3 documents\n",
      "    Time without reranker: 1.70s\n",
      "    Time with reranker: 130.83s\n",
      "  Top-k rank: 5\n",
      "    Sending request without reranker (top_k_rank=5)\n",
      "    Context retrieved without reranker: 5 documents\n",
      "    Sending request with reranker (top_k_rank=5)\n",
      "    Context retrieved with reranker: 5 documents\n",
      "    Time without reranker: 1.69s\n",
      "    Time with reranker: 129.14s\n",
      "Processing prompt: 'What potential applications does BigBird have in genomics data analysis?'\n",
      "  Top-k rank: 1\n",
      "    Sending request without reranker (top_k_rank=1)\n",
      "    Context retrieved without reranker: 1 documents\n",
      "    Sending request with reranker (top_k_rank=1)\n",
      "    Context retrieved with reranker: 1 documents\n",
      "    Time without reranker: 1.37s\n",
      "    Time with reranker: 120.32s\n",
      "  Top-k rank: 3\n",
      "    Sending request without reranker (top_k_rank=3)\n",
      "    Context retrieved without reranker: 3 documents\n",
      "    Sending request with reranker (top_k_rank=3)\n",
      "    Context retrieved with reranker: 3 documents\n",
      "    Time without reranker: 1.38s\n",
      "    Time with reranker: 120.94s\n",
      "  Top-k rank: 5\n",
      "    Sending request without reranker (top_k_rank=5)\n",
      "    Context retrieved without reranker: 5 documents\n",
      "    Sending request with reranker (top_k_rank=5)\n",
      "    Context retrieved with reranker: 5 documents\n",
      "    Time without reranker: 1.37s\n",
      "    Time with reranker: 120.10s\n",
      "Processing prompt: 'Explain how CANINE processes text directly at the Unicode character level without tokenization.'\n",
      "  Top-k rank: 1\n",
      "    Sending request without reranker (top_k_rank=1)\n",
      "    Context retrieved without reranker: 1 documents\n",
      "    Sending request with reranker (top_k_rank=1)\n",
      "    Context retrieved with reranker: 1 documents\n",
      "    Time without reranker: 1.53s\n",
      "    Time with reranker: 112.41s\n",
      "  Top-k rank: 3\n",
      "    Sending request without reranker (top_k_rank=3)\n",
      "    Context retrieved without reranker: 3 documents\n",
      "    Sending request with reranker (top_k_rank=3)\n",
      "    Context retrieved with reranker: 3 documents\n",
      "    Time without reranker: 1.62s\n",
      "    Time with reranker: 113.27s\n",
      "  Top-k rank: 5\n",
      "    Sending request without reranker (top_k_rank=5)\n",
      "    Context retrieved without reranker: 5 documents\n",
      "    Sending request with reranker (top_k_rank=5)\n",
      "    Context retrieved with reranker: 5 documents\n",
      "    Time without reranker: 1.73s\n",
      "    Time with reranker: 115.18s\n",
      "Processing prompt: 'How does CANINE compare to mBERT in terms of language adaptation and parameter efficiency?'\n",
      "  Top-k rank: 1\n",
      "    Sending request without reranker (top_k_rank=1)\n",
      "    Context retrieved without reranker: 1 documents\n",
      "    Sending request with reranker (top_k_rank=1)\n",
      "    Context retrieved with reranker: 1 documents\n",
      "    Time without reranker: 1.52s\n",
      "    Time with reranker: 118.14s\n",
      "  Top-k rank: 3\n",
      "    Sending request without reranker (top_k_rank=3)\n",
      "    Context retrieved without reranker: 3 documents\n",
      "    Sending request with reranker (top_k_rank=3)\n",
      "    Context retrieved with reranker: 3 documents\n",
      "    Time without reranker: 1.58s\n",
      "    Time with reranker: 122.27s\n",
      "  Top-k rank: 5\n",
      "    Sending request without reranker (top_k_rank=5)\n",
      "    Context retrieved without reranker: 5 documents\n",
      "    Sending request with reranker (top_k_rank=5)\n",
      "    Context retrieved with reranker: 5 documents\n",
      "    Time without reranker: 1.53s\n",
      "    Time with reranker: 119.30s\n",
      "Processing prompt: 'What are the key differences between CPM and GPT-2, particularly regarding Chinese NLP tasks?'\n",
      "  Top-k rank: 1\n",
      "    Sending request without reranker (top_k_rank=1)\n",
      "    Context retrieved without reranker: 1 documents\n",
      "    Sending request with reranker (top_k_rank=1)\n",
      "    Context retrieved with reranker: 1 documents\n",
      "    Time without reranker: 1.97s\n",
      "    Time with reranker: 116.75s\n",
      "  Top-k rank: 3\n",
      "    Sending request without reranker (top_k_rank=3)\n",
      "    Context retrieved without reranker: 3 documents\n",
      "    Sending request with reranker (top_k_rank=3)\n",
      "    Context retrieved with reranker: 3 documents\n",
      "    Time without reranker: 1.99s\n",
      "    Time with reranker: 117.25s\n",
      "  Top-k rank: 5\n",
      "    Sending request without reranker (top_k_rank=5)\n",
      "    Context retrieved without reranker: 5 documents\n",
      "    Sending request with reranker (top_k_rank=5)\n",
      "    Context retrieved with reranker: 5 documents\n",
      "    Time without reranker: 1.97s\n",
      "    Time with reranker: 114.65s\n",
      "Processing prompt: 'How does CPM excel in few-shot learning for Chinese language applications?'\n",
      "  Top-k rank: 1\n",
      "    Sending request without reranker (top_k_rank=1)\n",
      "    Context retrieved without reranker: 1 documents\n",
      "    Sending request with reranker (top_k_rank=1)\n",
      "    Context retrieved with reranker: 1 documents\n",
      "    Time without reranker: 1.52s\n",
      "    Time with reranker: 114.76s\n",
      "  Top-k rank: 3\n",
      "    Sending request without reranker (top_k_rank=3)\n",
      "    Context retrieved without reranker: 3 documents\n",
      "    Sending request with reranker (top_k_rank=3)\n",
      "    Context retrieved with reranker: 3 documents\n",
      "    Time without reranker: 1.55s\n",
      "    Time with reranker: 115.62s\n",
      "  Top-k rank: 5\n",
      "    Sending request without reranker (top_k_rank=5)\n",
      "    Context retrieved without reranker: 5 documents\n",
      "    Sending request with reranker (top_k_rank=5)\n",
      "    Context retrieved with reranker: 5 documents\n",
      "    Time without reranker: 1.66s\n",
      "    Time with reranker: 115.52s\n",
      "Processing prompt: 'What advantages does JetMoe-8B's sparsely activated architecture offer over dense language models?'\n",
      "  Top-k rank: 1\n",
      "    Sending request without reranker (top_k_rank=1)\n",
      "    Context retrieved without reranker: 1 documents\n",
      "    Sending request with reranker (top_k_rank=1)\n",
      "    Context retrieved with reranker: 1 documents\n",
      "    Time without reranker: 1.84s\n",
      "    Time with reranker: 118.81s\n",
      "  Top-k rank: 3\n",
      "    Sending request without reranker (top_k_rank=3)\n",
      "    Context retrieved without reranker: 3 documents\n",
      "    Sending request with reranker (top_k_rank=3)\n",
      "    Context retrieved with reranker: 3 documents\n",
      "    Time without reranker: 1.82s\n",
      "    Time with reranker: 122.50s\n",
      "  Top-k rank: 5\n",
      "    Sending request without reranker (top_k_rank=5)\n",
      "    Context retrieved without reranker: 5 documents\n",
      "    Sending request with reranker (top_k_rank=5)\n",
      "    Context retrieved with reranker: 5 documents\n",
      "    Time without reranker: 1.84s\n",
      "    Time with reranker: 124.73s\n",
      "Processing prompt: 'Describe the function of Mixture of Attention Heads and Mixture of MLP Experts in JetMoe-8B.'\n",
      "  Top-k rank: 1\n",
      "    Sending request without reranker (top_k_rank=1)\n",
      "    Context retrieved without reranker: 1 documents\n",
      "    Sending request with reranker (top_k_rank=1)\n",
      "    Context retrieved with reranker: 1 documents\n",
      "    Time without reranker: 1.98s\n",
      "    Time with reranker: 119.93s\n",
      "  Top-k rank: 3\n",
      "    Sending request without reranker (top_k_rank=3)\n",
      "    Context retrieved without reranker: 3 documents\n",
      "    Sending request with reranker (top_k_rank=3)\n",
      "    Context retrieved with reranker: 3 documents\n",
      "    Time without reranker: 1.98s\n",
      "    Time with reranker: 121.31s\n",
      "  Top-k rank: 5\n",
      "    Sending request without reranker (top_k_rank=5)\n",
      "    Context retrieved without reranker: 5 documents\n",
      "    Sending request with reranker (top_k_rank=5)\n",
      "    Context retrieved with reranker: 5 documents\n",
      "    Time without reranker: 1.98s\n",
      "    Time with reranker: 121.48s\n",
      "Completed all comparisons.\n",
      "File 'rag_comparisons.json' results saved to 'docs/reranked'.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion on the Use of a Reranker Based on the Results\n",
    "\n",
    "For example, the results for the last prompt in the file [result](docs/reranked/rag_comparisons.json) were used.\n",
    "\n",
    "The results from the three different queries with varying values of `top_k_rank` (1, 3, and 5) show some key insights regarding the impact of applying a reranker to the retrieval process.\n",
    "\n",
    "#### 1. **Effect on Context Retrieval:**\n",
    "- **No Significant Change in Retrieved Context**: In all tested cases, the retrieved contexts with and without the reranker appear to be very similar. For example, with a `top_k_rank` of 1, the context both with and without the reranker is identical. The same holds true for higher values of `top_k_rank` (3 and 5), where the top documents, although reordered slightly, remain largely the same in content.\n",
    "  \n",
    "  **Insight**: The reranker does not drastically change the core context or content of the documents, suggesting that its primary function may be to reorder already relevant content rather than introduce significantly different information.\n",
    "\n",
    "#### 2. **Response Time:**\n",
    "- **Significant Increase in Response Time**: The response time with the reranker applied is notably higher compared to without the reranker. For example:\n",
    "  - **top_k_rank=1**: The time increased from approximately 1.98 seconds (without reranker) to 119.93 seconds (with reranker).\n",
    "  - **top_k_rank=3**: The time increased from approximately 1.98 seconds (without reranker) to 121.31 seconds (with reranker).\n",
    "  - **top_k_rank=5**: The time increased from approximately 1.98 seconds (without reranker) to 121.48 seconds (with reranker).\n",
    "\n",
    "  **Insight**: The reranker significantly increases the computational cost, with times reaching over 60 times the original time without the reranker. This highlights a major trade-off between the quality of results and processing efficiency.\n",
    "\n",
    "#### 3. **Pros of Using a Reranker:**\n",
    "- **Improved Relevance of Retrieved Documents**: Although the specific content does not change drastically, the reranker may enhance the order in which the documents are retrieved, potentially placing more relevant content at the top. This can be beneficial in cases where the relevance ranking is crucial.\n",
    "  \n",
    "- **Better Alignment with User Queries**: In scenarios where the retrieval system’s ranking is suboptimal, a reranker can better align the retrieved documents with the user's query, improving the overall relevance.\n",
    "\n",
    "#### 4. **Cons of Using a Reranker:**\n",
    "- **Significant Increase in Response Time**: The main drawback of using a reranker is the substantial increase in processing time. For real-time applications or use cases requiring low-latency responses, this increase could be detrimental.\n",
    "  \n",
    "- **Limited Impact on Document Diversity**: The reranker does not seem to introduce much diversity in the retrieved content. The primary effect is reordering rather than enriching the set of documents retrieved, which might not be ideal when diverse perspectives or information are needed.\n",
    "\n",
    "#### 5. **Conclusion:**\n",
    "- **Usefulness in Specific Contexts**: The reranker can be a useful tool when the relevance of retrieved documents needs fine-tuning, especially when there are small differences in the quality of retrieved content. However, its high computational cost limits its practicality in scenarios where fast responses are needed, or when only slight improvements in relevance are required.\n",
    "  \n",
    "- **Recommendation**: For applications where response time is critical or the documents retrieved are already highly relevant, the reranker may not provide a justified return on investment due to its significant impact on processing time. In contrast, if the quality of the top-k documents is paramount, and response time is less of a concern, using the reranker can enhance the output quality significantly."
   ],
   "id": "3790b410b9229949"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. LLM Comparison",
   "id": "f6cd8089ec2d0180"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T11:58:06.966396Z",
     "start_time": "2024-12-12T11:58:03.591713Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install cohere",
   "id": "2ce84b8d02c1e447",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cohere\r\n",
      "  Downloading cohere-5.13.3-py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\r\n",
      "  Downloading fastavro-1.9.7-cp312-cp312-macosx_10_9_universal2.whl.metadata (5.5 kB)\r\n",
      "Requirement already satisfied: httpx>=0.21.2 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from cohere) (0.27.2)\r\n",
      "Requirement already satisfied: httpx-sse==0.4.0 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from cohere) (0.4.0)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.26 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from cohere) (1.26.4)\r\n",
      "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\r\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\r\n",
      "Requirement already satisfied: pydantic>=1.9.2 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from cohere) (2.10.1)\r\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from cohere) (2.27.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from cohere) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from cohere) (0.21.0)\r\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from cohere) (2.32.0.20241016)\r\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from cohere) (4.12.2)\r\n",
      "Requirement already satisfied: anyio in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (4.6.2.post1)\r\n",
      "Requirement already satisfied: certifi in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (2024.8.30)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (1.0.7)\r\n",
      "Requirement already satisfied: idna in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (3.10)\r\n",
      "Requirement already satisfied: sniffio in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (1.3.1)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from pydantic>=1.9.2->cohere) (0.7.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.0)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->cohere) (2.2.3)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from tokenizers<1,>=0.15->cohere) (0.26.2)\r\n",
      "Requirement already satisfied: filelock in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.16.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.9.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\r\n",
      "Downloading cohere-5.13.3-py3-none-any.whl (249 kB)\r\n",
      "Downloading fastavro-1.9.7-cp312-cp312-macosx_10_9_universal2.whl (1.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\r\n",
      "Installing collected packages: parameterized, fastavro, cohere\r\n",
      "Successfully installed cohere-5.13.3 fastavro-1.9.7 parameterized-0.9.0\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T12:58:08.012161Z",
     "start_time": "2024-12-12T12:56:25.412229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import cohere\n",
    "import openai\n",
    "\n",
    "# Список запросов\n",
    "prompts = [\n",
    "    \"What are the primary NLP tasks where BERTweet outperforms RoBERTa and XLM-R?\",\n",
    "    \"How does BERTweet differ from standard BERT in its implementation and tokenization methods?\",\n",
    "    \"How does BigBird's sparse attention mechanism achieve linear complexity for processing long sequences?\",\n",
    "    \"What potential applications does BigBird have in genomics data analysis?\",\n",
    "    \"Explain how CANINE processes text directly at the Unicode character level without tokenization.\",\n",
    "    \"How does CANINE compare to mBERT in terms of language adaptation and parameter efficiency?\",\n",
    "    \"What are the key differences between CPM and GPT-2, particularly regarding Chinese NLP tasks?\",\n",
    "    \"How does CPM excel in few-shot learning for Chinese language applications?\",\n",
    "    \"What advantages does JetMoe-8B's sparsely activated architecture offer over dense language models?\",\n",
    "    \"Describe the function of Mixture of Attention Heads and Mixture of MLP Experts in JetMoe-8B.\"\n",
    "]\n",
    "\n",
    "\n",
    "def openai_request(prompt):\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://api.studio.nebius.ai/v1/\",\n",
    "        api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.6,\n",
    "        max_tokens=150  \n",
    "    )\n",
    "    \n",
    "    summary = response.choices[0].message.content        \n",
    "    elapsed_time = time.time() - start_time\n",
    "        \n",
    "    return summary.strip(), elapsed_time\n",
    "\n",
    "\n",
    "def cohere_request(prompt):\n",
    "    co = cohere.Client(os.environ.get(\"COHERE_API_KEY\"))\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = co.generate(\n",
    "        model='command-r-plus-08-2024',  \n",
    "        prompt=prompt,\n",
    "        max_tokens=150  \n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return response.generations[0].text, elapsed_time\n",
    "\n",
    "\n",
    "def chatgpt_request(prompt):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\" ,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            max_tokens=150 \n",
    "        )\n",
    "            \n",
    "        summary = response.choices[0].message.content        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        return summary.strip(), elapsed_time\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with ChatGPT: {e}\")\n",
    "        return \"\", 0\n",
    "\n",
    "\n",
    "def compare_apis_for_prompt(prompt):\n",
    "    results = {}\n",
    "\n",
    "    openai_response, openai_time = openai_request(prompt)\n",
    "    results['OpenAI'] = {\n",
    "        'response': openai_response,\n",
    "        'time': openai_time\n",
    "    }\n",
    "    print(f\"OpenAI response time: {openai_time:.3f} seconds\")\n",
    "\n",
    " \n",
    "    cohere_response, cohere_time = cohere_request(prompt)\n",
    "    results['Cohere'] = {\n",
    "        'response': cohere_response,\n",
    "        'time': cohere_time\n",
    "    }\n",
    "    print(f\"Cohere response time: {openai_time:.3f} seconds\")\n",
    "\n",
    "\n",
    "    chatgpt_response, chatgpt_time = chatgpt_request(prompt)\n",
    "    results['ChatGPT'] = {\n",
    "        'response': chatgpt_response,\n",
    "        'time': chatgpt_time\n",
    "    }\n",
    "    print(f\"ChatGPT response time: {openai_time:.3f} seconds\")\n",
    "\n",
    "    return {\n",
    "        \"query\": prompt,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "\n",
    "def save_all_results_to_json(prompts, filename='api_comparison_results.json'):\n",
    "    directory = 'docs/compared'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    file_path = os.path.join(directory, filename)\n",
    "    \n",
    "    all_results = []\n",
    "    for prompt in prompts:\n",
    "        result = compare_apis_for_prompt(prompt)\n",
    "        all_results.append(result)\n",
    "\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "\n",
    "    print(f\"Results have been saved to {file_path}\")\n",
    "\n",
    "\n",
    "save_all_results_to_json(prompts)"
   ],
   "id": "a5632ff5ace0187",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI response time: 5.464 seconds\n",
      "Cohere response time: 5.464 seconds\n",
      "ChatGPT response time: 5.464 seconds\n",
      "OpenAI response time: 4.390 seconds\n",
      "Cohere response time: 4.390 seconds\n",
      "ChatGPT response time: 4.390 seconds\n",
      "OpenAI response time: 7.207 seconds\n",
      "Cohere response time: 7.207 seconds\n",
      "ChatGPT response time: 7.207 seconds\n",
      "OpenAI response time: 4.714 seconds\n",
      "Cohere response time: 4.714 seconds\n",
      "ChatGPT response time: 4.714 seconds\n",
      "OpenAI response time: 3.722 seconds\n",
      "Cohere response time: 3.722 seconds\n",
      "ChatGPT response time: 3.722 seconds\n",
      "OpenAI response time: 3.639 seconds\n",
      "Cohere response time: 3.639 seconds\n",
      "ChatGPT response time: 3.639 seconds\n",
      "OpenAI response time: 3.554 seconds\n",
      "Cohere response time: 3.554 seconds\n",
      "ChatGPT response time: 3.554 seconds\n",
      "OpenAI response time: 4.362 seconds\n",
      "Cohere response time: 4.362 seconds\n",
      "ChatGPT response time: 4.362 seconds\n",
      "OpenAI response time: 3.682 seconds\n",
      "Cohere response time: 3.682 seconds\n",
      "ChatGPT response time: 3.682 seconds\n",
      "OpenAI response time: 3.703 seconds\n",
      "Cohere response time: 3.703 seconds\n",
      "ChatGPT response time: 3.703 seconds\n",
      "Results have been saved to docs/compared/api_comparison_results.json\n",
      "Results have been saved to api_comparison_results.json\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion on the API Comparison\n",
    "\n",
    "The conclusion was made based on the prompt: **\"What are the primary NLP tasks where BERTweet outperforms RoBERTa and XLM-R?\"**\n",
    "The results are here [result](docs/compared/api_comparison_results.json).\n",
    "\n",
    "This translation keeps the focus on the specific query from which the analysis was derived.\n",
    "\n",
    "From the provided output, several important conclusions can be drawn regarding the performance of three different models: **OpenAI**, **Cohere**, and **ChatGPT**.\n",
    "\n",
    "### 1. **Comparison of Responses to the Query**\n",
    "All three models provided similar responses, highlighting the primary areas where **BERTweet** outperforms **RoBERTa** and **XLM-R**:\n",
    "\n",
    "- **OpenAI**: Mentions tweet classification tasks such as sentiment analysis, hate speech detection, and topic modeling. It also highlights Named Entity Recognition (NER).\n",
    "  \n",
    "- **Cohere**: Focuses on sentiment analysis and the presence of Twitter-specific language, hashtags, and emojis, emphasizing the importance of pre-training on a large Twitter corpus.\n",
    "\n",
    "- **ChatGPT**: Also mentions sentiment analysis but adds other tasks like emotion recognition and toxicity detection, reflecting a broader perspective on BERTweet’s application.\n",
    "\n",
    "### 2. **Similarities in Responses**\n",
    "All models agree on the following points:\n",
    "- **BERTweet** is a specialized model for analyzing social media text, such as tweets, and shows advantages in tasks related to sentiment analysis and texts containing specific elements of social networks (emojis, hashtags, slang).\n",
    "- Both **RoBERTa** and **XLM-R** are more general-purpose models that lack the specialization for social media text.\n",
    "\n",
    "### 3. **Differences in Focus**\n",
    "- **OpenAI** focuses on **Named Entity Recognition (NER)** and tweet classification.\n",
    "- **Cohere** emphasizes **sentiment analysis** and working with **social media signals** (such as hashtags and emojis).\n",
    "- **ChatGPT** highlights additional tasks like **emotion recognition** and **toxicity detection**, which may be more relevant for social media analysis.\n",
    "\n",
    "### 4. **Response Time**\n",
    "- **OpenAI**: 3.53 seconds\n",
    "- **Cohere**: 3.35 seconds\n",
    "- **ChatGPT**: 2.99 seconds\n",
    "\n",
    "**Conclusions**:\n",
    "- All three models provided responses quickly, with minor differences in execution time. **ChatGPT** demonstrated the fastest response time (2.99 seconds), which can be valuable for applications requiring speed.\n",
    "- **Cohere** and **OpenAI** showed similar response times, but **Cohere** was slightly faster than **OpenAI** (3.35 vs. 3.53 seconds).\n",
    "\n",
    "### 5. **Conclusion**\n",
    "- All models demonstrated a solid understanding of **BERTweet** and highlighted its key advantages in tasks related to social media.\n",
    "- Despite the similarity in responses, the slight differences in focus and proposed tasks may reflect the unique characteristics of each model.\n",
    "- The response times were not significantly different, suggesting that each model is fairly efficient for this type of query.\n",
    "\n",
    "Depending on the context of your task, you may choose the most suitable model:\n",
    "- **ChatGPT** may be the preferred choice if response speed is important.\n",
    "- **OpenAI** and **Cohere** could be more useful if you need more detailed responses, particularly for sentiment analysis and social media text tasks.\n"
   ],
   "id": "dc06edad176c4002"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Evaluation Setup",
   "id": "fa856aaf7471381d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T13:21:22.463298Z",
     "start_time": "2024-12-12T13:21:21.696783Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install huggingface_hub pandas tqdm",
   "id": "dee46341d61cd86e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (0.26.2)\r\n",
      "Requirement already satisfied: pandas in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: tqdm in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (4.67.1)\r\n",
      "Requirement already satisfied: filelock in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface_hub) (3.16.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface_hub) (2024.9.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface_hub) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from huggingface_hub) (4.12.2)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from pandas) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from pandas) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from pandas) (2024.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from requests->huggingface_hub) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from requests->huggingface_hub) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alexey.potapov/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.8.30)\r\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:41:54.773499Z",
     "start_time": "2024-12-12T15:39:25.973412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"  # Модель для оценки\n",
    "llm_client = InferenceClient(model=repo_id, timeout=120, token=os.environ.get(\"HUG_API_KEY\"))\n",
    "\n",
    "def extract_judge_score(response):\n",
    "    match = re.findall(r'Total rating: (\\d+)', response)\n",
    "    if match:\n",
    "        return int(match[-1])\n",
    "    return None\n",
    "\n",
    "def evaluate_response(question, answer, retries=5, delay=10):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            prompt = f\"\"\"            \n",
    "Evaluate the following response to the query:\n",
    "\"QUERY: {question}\"\n",
    "\"RESPONSE: {answer}\"\n",
    "Provide a 'Total rating' from 1 to 10 based on relevance, coherence, depth, and accuracy. \n",
    "Justify the score with specific details about the response.\n",
    "\n",
    "Your feedback should be structured as follows:\n",
    "Total rating: (your rating, as a number between 1 and 10)       \n",
    "\"\"\"\n",
    "            response = llm_client.text_generation(\n",
    "                prompt=prompt.format(question=[\"question\"], answer=[\"answer\"]),\n",
    "                 max_new_tokens=1000,\n",
    "             )\n",
    "            return extract_judge_score(response.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt + 1}: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)  \n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping this response.\")\n",
    "                return None\n",
    "\n",
    "\n",
    "# file_path = \"docs/compared/api_comparison_results.json\"\n",
    "file_path = \"docs/compared/low_score.json\"\n",
    "\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for entry in tqdm(data, desc=\"Processing entries\"):\n",
    "    query = entry.get(\"query\", \"No query provided\")  \n",
    "    results = entry.get(\"results\", {})  \n",
    "    \n",
    "    for model_name, model_data in results.items():\n",
    "        response = model_data.get(\"response\", \"No response\")  \n",
    "        time_taken = model_data.get(\"time\", None)  \n",
    "        try:\n",
    "\n",
    "            score = evaluate_response(query, response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating response for model {model_name}: {e}\")\n",
    "            score = None\n",
    "        \n",
    "\n",
    "        evaluation_results.append({\n",
    "            \"query\": query,\n",
    "            \"model\": model_name,\n",
    "            \"response\": response,\n",
    "            \"time_taken\": time_taken,\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "output_file=\"docs/evaluated/evaluation_low_results.json\"\n",
    "# output_file = \"docs/evaluated/evaluation_results.json\"\n",
    "with open(output_file, 'w') as json_file:\n",
    "    json.dump(evaluation_results, json_file, indent=4)\n",
    "\n",
    "print(f\"Evaluation completed. Results saved to {output_file}\")"
   ],
   "id": "6e67b72edb15c759",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entries: 100%|██████████| 5/5 [02:28<00:00, 29.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed. Results saved to docs/evaluated/evaluation_low_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Summary and Conclusions from Evaluation**\n",
    "\n",
    "In this evaluation, two distinct datasets were used to assess the performance of responses from different language models (LLMs) such as Cohere, OpenAI, and ChatGPT. The evaluation focused on how well the models addressed queries in the fields of NLP, specifically concerning model architectures like BERTweet, CANINE, and others.\n",
    "\n",
    "Here are the files containing the evaluation results:\n",
    "- [low_score](docs/evaluated/evaluation_low_results.json)\n",
    "- [high_score](docs/evaluated/evaluation_results.json)\n",
    "\n",
    "\n",
    "#### **Key Findings from the Evaluation:**\n",
    "\n",
    "1. **Relevance and Coherence:**\n",
    "   - Responses from the **Cohere**, **OpenAI**, and **ChatGPT** models generally stayed relevant to the queries, though there were instances where the answers provided lacked specificity or were less detailed, particularly in cases involving complex concepts like tokenization methods or character-level processing (e.g., for BERTweet).\n",
    "   - The **ChatGPT** responses tended to be more coherent in structuring information logically. Some responses, however, lacked depth or clear explanation (such as in the responses related to BERTweet and tokenization differences).\n",
    "\n",
    "2. **Depth of Response:**\n",
    "   - The **ChatGPT** responses generally had more detail in the explanations, providing a deeper understanding of complex topics (e.g., **CANINE** and **mBERT** comparisons). These responses often included clear breakdowns of technical characteristics.\n",
    "   - **Cohere** and **OpenAI** responses, while generally accurate, sometimes showed a lack of detailed coverage. For example, in the case of **BERTweet**, the responses did not provide sufficient information about tokenization or implementation differences, resulting in lower scores for depth.\n",
    "   - In the case of **CANINE**, the **ChatGPT** and **OpenAI** responses were much more comprehensive, clearly discussing the character-level processing advantages of CANINE. **Cohere's** response was somewhat superficial and incomplete, which resulted in lower depth ratings.\n",
    "\n",
    "3. **Accuracy:**\n",
    "   - **ChatGPT** demonstrated a high level of accuracy across responses. It correctly described the key features of models like **CANINE** and **mBERT**, explaining their differences in terms of language adaptation and parameter efficiency.\n",
    "   - **Cohere**'s and **OpenAI**'s responses were accurate but sometimes missed important technical details or nuances. For example, **Cohere** sometimes provided more general statements (e.g., regarding **BERTweet** or **mBERT**) without diving deeply into the specific technical distinctions.\n",
    "\n",
    "4. **Comparison of Performance between the Two Datasets:**\n",
    "   - **First Dataset (LLM responses from the previous task)**:\n",
    "     The first set of responses from **Cohere**, **OpenAI**, and **ChatGPT** typically demonstrated a higher quality, particularly in terms of relevance and depth. These models were more precise in explaining complex NLP concepts and demonstrated a clear understanding of how advanced models operate.\n",
    "   \n",
    "   - **Second Dataset (Artificially generated responses with lower quality)**:\n",
    "     The second set of responses, artificially generated with low quality, were often brief, vague, and lacking critical information. The responses failed to provide sufficient depth, and in some cases, accuracy was questionable. For example, responses about **BERTweet** lacked specific details about tokenization methods, and some descriptions about **CANINE** were imprecise, which contributed to lower scores for these responses.\n",
    "\n",
    "#### **Scores Overview:**\n",
    "\n",
    "- **High-Quality Responses**: \n",
    "  - **ChatGPT** stood out in this dataset, often receiving ratings of 8-9 for its detailed and accurate answers, especially for queries about **CANINE** and **mBERT**.\n",
    "  - **OpenAI** also provided solid answers, with ratings mostly around 7-8 for its coverage of technical topics.\n",
    "  \n",
    "- **Lower-Quality Responses**: \n",
    "  - **Cohere**'s responses were often rated lower (4-6). These responses were sometimes vague, missing depth, or only partially accurate. For example, its answer regarding **BERTweet** tokenization showed a lack of technical depth, resulting in a lower score.\n",
    "\n",
    "#### **Conclusions:**\n",
    "\n",
    "1. **Quality of Responses**:\n",
    "   - **ChatGPT** was the most consistent and detailed in its responses, providing comprehensive and accurate explanations. It demonstrated a higher level of understanding when addressing complex topics, such as tokenization and model architecture differences.\n",
    "   - **Cohere**'s responses often lacked detail, and while it occasionally offered accurate information, it failed to provide the depth needed for higher scores.\n",
    "   - **OpenAI** performed better than **Cohere**, but sometimes struggled with the depth and clarity that **ChatGPT** provided.\n",
    "\n",
    "2. **Implications for Model Selection**:\n",
    "   - When seeking detailed and accurate information on complex NLP topics, **ChatGPT** performed the best and would be the most reliable model to rely on for high-quality outputs.\n",
    "   - **Cohere** may need further fine-tuning or more detailed training data to improve the specificity and depth of its responses.\n",
    "   - **OpenAI** performed well but could benefit from providing more detailed explanations, especially when dealing with technical differences in model architectures.\n",
    "\n",
    "In summary, **ChatGPT** generally provided the most accurate, relevant, and coherent answers, particularly when complex NLP topics were discussed. The other models, while still effective, showed limitations in their ability to handle depth and specificity."
   ],
   "id": "7404bd336b3a7fe1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
